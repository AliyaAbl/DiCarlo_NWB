{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook was used to create a standartized inventory for pico, including the spiketimes and psth h5 files. additionally, the information at hand was extracted from the excel file and saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd\n",
    "import os\n",
    "import fnmatch\n",
    "import shutil\n",
    "import re\n",
    "from ruamel.yaml import YAML\n",
    "import scipy.io as sio\n",
    "\n",
    "def find_directories_with_name(root_dir, directory_name):\n",
    "    directory_paths = []\n",
    "    for foldername, subfolders, _ in os.walk(root_dir):\n",
    "        # print(foldername, subfolders, _)\n",
    "        if directory_name in subfolders:\n",
    "            added = True\n",
    "            for subsubfolder in os.listdir(os.path.join(foldername, directory_name)):\n",
    "                if os.path.isdir(os.path.join(foldername, directory_name, subsubfolder)):\n",
    "                    directory_paths.append(os.path.join(foldername, directory_name, subsubfolder))\n",
    "                else: added = False\n",
    "            if added == False: directory_paths.append(os.path.join(foldername, directory_name))\n",
    "\n",
    "    return directory_paths\n",
    "\n",
    "def find_files_with_extension(root_dir, extension):\n",
    "    file_paths = []\n",
    "    for foldername, subfolders, filenames in os.walk(root_dir):\n",
    "        for filename in fnmatch.filter(filenames, f'*.{extension}'):\n",
    "            file_paths.append(os.path.join(foldername, filename))\n",
    "    return file_paths\n",
    "\n",
    "def get_date(date):\n",
    "    \"\"\"\n",
    "    Get the Dates from the Excel file.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dates = date.split('/')\n",
    "        if len(dates[-1])>2:year = dates[-1][2:]\n",
    "        else: year=f'{dates[-1]}'\n",
    "        if len(dates[0])>1: month = dates[0]\n",
    "        else: month = f'0{dates[0]}'\n",
    "        if len(dates[1])>1:day=dates[1]\n",
    "        else:day=f'0{dates[1]}'\n",
    "        \n",
    "\n",
    "    except:\n",
    "        try:\n",
    "            dates = str(date).split()[0].split('-')\n",
    "            if len(dates)==3:\n",
    "                year=dates[0][2:]\n",
    "                month=dates[1]\n",
    "                if len(dates[2])>1:day=dates[2]\n",
    "                else:day=f'0{dates[2]}'\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try: return day, month, year\n",
    "    except: return 0,0,0\n",
    "  \n",
    "def find_files_with_pattern(root_dir, pattern):\n",
    "    \"\"\"\n",
    "    Find Associated .h5 File for each SpikeTime Folder.\n",
    "    \"\"\"\n",
    "    return [os.path.join(foldername, filename) for foldername, subfolders, filenames in os.walk(root_dir) for filename in filenames if filename.startswith(pattern) and filename.endswith('.h5')]\n",
    "\n",
    "def combine_channels(proc_dir, savedir, num_channels):\n",
    "\n",
    "    filename = proc_dir.split('/')[-2]\n",
    "    ch_files = os.listdir(proc_dir)\n",
    "    ch_files.sort()\n",
    "    if not len(ch_files) == num_channels:  # Skip if not all channel files present\n",
    "        print('RERUN, not all channel files are present!')\n",
    "    ch_files = [item for item in ch_files if not item.startswith('.')]\n",
    "    psth = [sio.loadmat(os.path.join(proc_dir, f), squeeze_me=True, variable_names='psth')['psth'] for f in ch_files]\n",
    "    psth = np.asarray(psth)  # channels x stimuli x reps x timebins\n",
    "    psth = np.moveaxis(psth, 0, -1)  # stimuli x reps x timebins x channels\n",
    "    #logging.debug(psth.shape)\n",
    "\n",
    "    meta = sio.loadmat(os.path.join(proc_dir, ch_files[0]), squeeze_me=True, variable_names='meta')['meta']\n",
    "    psth = {'psth': psth, 'meta': meta}\n",
    "    sio.savemat(os.path.join(savedir, filename+'_psth.mat'), psth)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "inventory_excel_file_name = 'pico_inventory.xlsx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Find directories in Sarah's braintree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Find Directories of SpikeTimes ################################\n",
    "###############################################################################\n",
    "parent_directory = '/braintree/data2/active/users/sgouldin/projects'\n",
    "target_directory_name = 'spikeTime'\n",
    "spikeTime_directories = find_directories_with_name(parent_directory, target_directory_name)\n",
    "print(len(spikeTime_directories))\n",
    "\n",
    "spike_parent = []\n",
    "for path in spikeTime_directories:\n",
    "    try: \n",
    "        if path.split('/')[9] == 'pico':\n",
    "            spike_parent.append(os.path.join(*path.split('/')[0:-1]))\n",
    "    except: pass\n",
    "    #spike_parent.append(os.path.join(*path.split('/')[0:10]))\n",
    "\n",
    "############### Find Directories of psth ######################################\n",
    "###############################################################################\n",
    "psth_directories = find_directories_with_name(parent_directory, 'psth')\n",
    "print(len(psth_directories))\n",
    "psth_parent = []\n",
    "for path in psth_directories:\n",
    "    try: \n",
    "        if path.split('/')[9] == 'pico':\n",
    "            psth_parent.append(os.path.join(*path.split('/')[0:-1]))\n",
    "    except: pass\n",
    "\n",
    "############### Find Directories of IntanRaw ##################################\n",
    "###############################################################################\n",
    "target_directory_name = 'intanraw'\n",
    "intanRaw_directories = find_directories_with_name(parent_directory, target_directory_name)\n",
    "print(len(intanRaw_directories))\n",
    "\n",
    "intanRaw_parents = []\n",
    "for path in intanRaw_directories:\n",
    "    try: \n",
    "        if path.split('/')[9] == 'pico':\n",
    "            intanRaw_parents.append(os.path.join(*path.split('/')[10]))\n",
    "    except: pass #print(path)\n",
    "    # intanRaw_parents.append(os.path.join(*path.split('/')[10]))\n",
    "\n",
    "\n",
    "############### Find Directories for PSTH h5 Files ############################\n",
    "###############################################################################\n",
    "extension = 'h5'\n",
    "h5_directories = find_files_with_extension(parent_directory, extension)\n",
    "print(len(h5_directories))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'braintree/data2/active/users/sgouldin/projects/NSD-COCO/monkeys/pico/intanproc/pico_NSD-COCO_230313_133413',\n",
       " 'braintree/data2/active/users/sgouldin/projects/NSD-COCO/monkeys/pico/intanproc/pico_NSD-COCO_230314_111731',\n",
       " 'braintree/data2/active/users/sgouldin/projects/NSD-COCO/monkeys/pico/intanproc/pico_NSD-COCO_230315_120952',\n",
       " 'braintree/data2/active/users/sgouldin/projects/gestalt/monkeys/pico/intanproc/pico_gestalt_230503_132701',\n",
       " 'braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanproc/pico_normalizers_220705_143816',\n",
       " 'braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanproc/pico_normalizers_220706_142235',\n",
       " 'braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanproc/pico_normalizers_230804_152622',\n",
       " 'braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanproc/pico_normalizers_230808_132257',\n",
       " 'braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanproc/pico_normalizers_230809_120658',\n",
       " 'braintree/data2/active/users/sgouldin/projects/object_relations/monkeys/pico/intanproc/pico_object_relations_230404_143641',\n",
       " 'braintree/data2/active/users/sgouldin/projects/object_relations/monkeys/pico/intanproc/pico_object_relations_230405_113610',\n",
       " 'braintree/data2/active/users/sgouldin/projects/object_relations/monkeys/pico/intanproc/pico_object_relations_230406_115302',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230412_113829',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230413_122224',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230414_114440',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230418_123135',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230419_124802',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230420_121445',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230421_124925',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230424_121602',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230426_130453',\n",
       " 'braintree/data2/active/users/sgouldin/projects/physion/monkeys/pico/intanproc/pico_physion_230427_114754',\n",
       " 'braintree/data2/active/users/sgouldin/projects/shapegen_dynamic/monkeys/pico/intanproc/pico_shapegen_dynamic_230316_121921',\n",
       " 'braintree/data2/active/users/sgouldin/projects/shapegen_dynamic/monkeys/pico/intanproc/pico_shapegen_dynamic_230320_124121'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(spike_parent) - set(psth_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Get Info for Pico Data ########################################\n",
    "###############################################################################\n",
    "\n",
    "intan_names = []\n",
    "names = []\n",
    "dates = []\n",
    "times = []\n",
    "pico_intanRaw_directories  = []\n",
    "\n",
    "for path in intanRaw_directories:\n",
    "    try:\n",
    "        if path.split('/')[9] == 'pico': \n",
    "            try: \n",
    "                intan_names.append(path.split('/')[11])\n",
    "                dates.append((path.split('/')[11].split('_')[-2]))\n",
    "                times.append((path.split('/')[11].split('_')[-1]))\n",
    "                names.append(path.split('/')[7])\n",
    "                pico_intanRaw_directories.append(path)\n",
    "            except: pass\n",
    "    except: pass\n",
    "\n",
    "spike_names = []\n",
    "pico_spikeTime_directories = []\n",
    "for path in spikeTime_directories:\n",
    "    try:\n",
    "        if path.split('/')[9] == 'pico':\n",
    "            spike_names.append(path.split('/')[11])\n",
    "            pico_spikeTime_directories.append(path)\n",
    "    except: pass #print(path)\n",
    "\n",
    "\n",
    "psth_names = []\n",
    "pico_psth_directories = []\n",
    "for path in psth_directories:\n",
    "    try:\n",
    "        if path.split('/')[9] == 'pico':\n",
    "            # for ele in os.listdir(os.path.join('/', *path.split('/')[0:-1])):\n",
    "            #     if ele.endswith(\".mat\"): \n",
    "            #         psth_names.append(path.split('/')[11])\n",
    "            #         path_ = os.path.join(os.path.join('/', *path.split('/')[0:-1], ele))\n",
    "            pico_psth_directories.append(path)\n",
    "            psth_names.append(path.split('/')[11])\n",
    "    except: pass #print(path)\n",
    "\n",
    "\n",
    "h5_names = []\n",
    "h5_dates = []\n",
    "pico_h5_directories = []\n",
    "for path in h5_directories:\n",
    "    try:\n",
    "        if path.split('/')[9] == 'pico':\n",
    "            h5_names.append(path.split('/')[7])\n",
    "            h5_dates.append(path.split('/')[11].split('.')[0])\n",
    "            pico_h5_directories.append(path)\n",
    "    except: pass #print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Write inventory excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Excel:  220719 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_220719_140828\n",
      "No Excel:  230922 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_230922_134247\n",
      "No Excel:  230929 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_230929_095756\n",
      "No Excel:  230106 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_230106_162941\n",
      "No Excel:  230920 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_230920_111320\n",
      "No Excel:  230925 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_230925_111712\n",
      "No Excel:  230927 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_230927_100801\n",
      "No Excel:  230928 normalizers /braintree/data2/active/users/sgouldin/projects/normalizers/monkeys/pico/intanraw/pico_normalizers_230928_101016\n",
      "No Excel:  220719 domain-transfer /braintree/data2/active/users/sgouldin/projects/domain-transfer/monkeys/pico/intanraw/pico_domain-transfer_220719_142555\n",
      "No Excel:  230106 test_project1 /braintree/data2/active/users/sgouldin/projects/test_project1/monkeys/pico/intanraw/pico_test_project1_230106_164343\n",
      "No Excel:  220719 HVM-var6 /braintree/data2/active/users/sgouldin/projects/HVM-var6/monkeys/pico/intanraw/pico_HVM-var6_220719_153937\n",
      "No Excel:  230928 robustness_guy_d1_v40 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v40/monkeys/pico/intanraw/pico_robustness_guy_d1_v40_230928_102556\n",
      "No Excel:  230928 robustness_guy_d1_v40 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v40/monkeys/pico/intanraw/pico_robustness_guy_d1_v40_230928_124707\n",
      "No Excel:  230928 robustness_guy_d1_v40 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v40/monkeys/pico/intanraw/pico_robustness_guy_d1_v40_230928_131702\n",
      "No Excel:  220804 normalizers-HVM /braintree/data2/active/users/sgouldin/projects/normalizers-HVM/monkeys/pico/intanraw/pico_normalizers_220804_152840\n",
      "No Excel:  220805 normalizers-HVM /braintree/data2/active/users/sgouldin/projects/normalizers-HVM/monkeys/pico/intanraw/pico_normalizers_220805_111903\n",
      "No Excel:  220808 normalizers-HVM /braintree/data2/active/users/sgouldin/projects/normalizers-HVM/monkeys/pico/intanraw/pico_normalizers_220808_141144\n",
      "No Excel:  230809 findfix /braintree/data2/active/users/sgouldin/projects/findfix/monkeys/pico/intanraw/pico_findfix_230809_122324\n",
      "No Excel:  230809 findfix /braintree/data2/active/users/sgouldin/projects/findfix/monkeys/pico/intanraw/pico_findfix_230809_124213\n",
      "No Excel:  230823 findfix /braintree/data2/active/users/sgouldin/projects/findfix/monkeys/pico/intanraw/pico_findfix_230823_125830\n",
      "No Excel:  230920 robustness_guy_d1_v37 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v37/monkeys/pico/intanraw/pico_robustness_d1_v37_230920_112814\n",
      "No Excel:  230922 robustness_guy_d1_v39 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v39/monkeys/pico/intanraw/pico_robustness_guy_d1_v39_230922_163334\n",
      "No Excel:  230922 robustness_guy_d1_v39 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v39/monkeys/pico/intanraw/pico_robustness_guy_d1_v39_230922_135839\n",
      "No Excel:  230925 afv /braintree/data2/active/users/sgouldin/projects/afv/monkeys/pico/intanraw/pico_afv_230925_120343\n",
      "No Excel:  230925 Kar_2023_degraded /braintree/data2/active/users/sgouldin/projects/Kar_2023_degraded/monkeys/pico/intanraw/pico_Kar_2023_degraded_230925_113603\n",
      "No Excel:  230927 robustness_guy_d0_v40 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d0_v40/monkeys/pico/intanraw/pico_robustness_guy_d0_v40_230927_102628\n",
      "No Excel:  230929 robustness_guy_d1_v41 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v41/monkeys/pico/intanraw/pico_robustness_guy_d1_v41_230929_125427\n",
      "No Excel:  230929 robustness_guy_d1_v41 /braintree/data2/active/users/sgouldin/projects/robustness_guy_d1_v41/monkeys/pico/intanraw/pico_robustness_guy_d1_v41_230929_102605\n",
      "No Excel:  230608 IAPS-200on /braintree/data2/active/users/sgouldin/projects/IAPS-200on/monkeys/pico/intanraw/pico_IAPS_200on_230608_152356\n",
      "No Excel:  230609 IAPS-200on /braintree/data2/active/users/sgouldin/projects/IAPS-200on/monkeys/pico/intanraw/pico_IAPS_200on_230609_134617\n",
      "No Excel:  230316 shapegen_static /braintree/data2/active/users/sgouldin/projects/shapegen_static/monkeys/pico/intanraw/pico_shapegen_static_230316_130233\n",
      "No Excel:  230320 shapegen_static /braintree/data2/active/users/sgouldin/projects/shapegen_static/monkeys/pico/intanraw/pico_shapegen_static_230320_132427\n",
      "No Excel:  230316 shapegen_dynamic /braintree/data2/active/users/sgouldin/projects/shapegen_dynamic/monkeys/pico/intanraw/pico_shapegen_dynamic_230316_121921\n",
      "No Excel:  230320 shapegen_dynamic /braintree/data2/active/users/sgouldin/projects/shapegen_dynamic/monkeys/pico/intanraw/pico_shapegen_dynamic_230320_124121\n",
      "No Excel:  230511 oasis900_200on /braintree/data2/active/users/sgouldin/projects/oasis900_200on/monkeys/pico/intanraw/pico_oasis900_200on_230511_122443\n",
      "No Excel:  230825 findfix_oasis /braintree/data2/active/users/sgouldin/projects/findfix_oasis/monkeys/pico/intanraw/pico_findfix_oasis_230825_101541\n",
      "No Excel:  230830 findfix_oasis /braintree/data2/active/users/sgouldin/projects/findfix_oasis/monkeys/pico/intanraw/pico_findfix_oasis_230830_112640\n",
      "No Excel:  230821 oasis900scrambled_200on /braintree/data2/active/users/sgouldin/projects/oasis900scrambled_200on/monkeys/pico/intanraw/pico_oasis900scrambled_230821_121635\n",
      "No Excel:  230822 oasis900scrambled_200on /braintree/data2/active/users/sgouldin/projects/oasis900scrambled_200on/monkeys/pico/intanraw/pico_oasis900scrambled_230822_155903\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageSet</th>\n",
       "      <th>Num Rec. Sess.</th>\n",
       "      <th>Num Has SpikeTime</th>\n",
       "      <th>Num Has psth</th>\n",
       "      <th>Num Has h5</th>\n",
       "      <th>Num Has Excel</th>\n",
       "      <th>BrainScore</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>normalizers</td>\n",
       "      <td>168</td>\n",
       "      <td>127</td>\n",
       "      <td>122</td>\n",
       "      <td>93</td>\n",
       "      <td>160</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>normalizers-HVM</td>\n",
       "      <td>34</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>mayo</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>muri1320</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>domain-transfer-2023</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ImageSet  Num Rec. Sess.  Num Has SpikeTime  Num Has psth  \\\n",
       "53           normalizers             168                127           122   \n",
       "54       normalizers-HVM              34                 20            20   \n",
       "36                  mayo              14                 14            14   \n",
       "50              muri1320              12                 12            12   \n",
       "17  domain-transfer-2023              12                 12            12   \n",
       "\n",
       "    Num Has h5  Num Has Excel BrainScore Comments  \n",
       "53          93            160                      \n",
       "54          20             31                      \n",
       "36          10             14                      \n",
       "50          12             12                      \n",
       "17          12             12                      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "############### Create DataFrame ##############################################\n",
    "###############################################################################\n",
    "df = pd.DataFrame({'ImageSet': names})\n",
    "df['date'] = dates\n",
    "df['time'] = times\n",
    "df['Has SpikeTime']     = [0]*len(pico_intanRaw_directories)\n",
    "df['Has psth']     = [0]*len(pico_intanRaw_directories)\n",
    "df['Has h5']            = [0]*len(pico_intanRaw_directories)\n",
    "df['Has Excel']         = [0]*len(pico_intanRaw_directories)\n",
    "df['(excel) Index']          = ['']*len(pico_intanRaw_directories)\n",
    "df['(excel) Stimuli']          = ['']*len(pico_intanRaw_directories)\n",
    "df['(excel) Num Reps']          = ['']*len(pico_intanRaw_directories)\n",
    "df['(excel) Num Images']        = ['']*len(pico_intanRaw_directories)\n",
    "df['(excel) Total Num Images']  = ['']*len(pico_intanRaw_directories)\n",
    "df['(excel) Notes']  = ['']*len(pico_intanRaw_directories)\n",
    "df['Path: SpikeTimes']  = ['']*len(pico_intanRaw_directories)\n",
    "df['Path: psth']  = ['']*len(pico_intanRaw_directories)\n",
    "df['Path: h5']          = ['']*len(pico_intanRaw_directories)\n",
    "df['Path: intanraw']    =  pico_intanRaw_directories\n",
    "\n",
    "indices_spiketime = [index for index, name in enumerate(intan_names) if name in spike_names]\n",
    "for ind in indices_spiketime:\n",
    "    df.at[ind, 'Has SpikeTime'] = 1\n",
    "    df.at[ind, 'Path: SpikeTimes'] = pico_spikeTime_directories[spike_names.index(intan_names[ind])]\n",
    "\n",
    "indices_psth = [index for index, name in enumerate(intan_names) if name in psth_names]\n",
    "for ind in indices_psth:\n",
    "    df.at[ind, 'Has psth'] = 1\n",
    "    df.at[ind, 'Path: psth'] = pico_psth_directories[psth_names.index(intan_names[ind])]\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for path, date in zip(pico_intanRaw_directories, dates):\n",
    "    matching_h5files = find_files_with_pattern(os.path.join('/', *path.split('/')[0:10]), date)\n",
    "    if len(matching_h5files) == 1:\n",
    "        df.at[i, 'Path: h5'] = matching_h5files[0]\n",
    "        df.at[i, 'Has h5'] = 1\n",
    "    if len(matching_h5files) > 1:\n",
    "        df.at[i, 'Path: h5'] = f'{len(matching_h5files)} files: {matching_h5files}'\n",
    "        df.at[i, 'Has h5'] = 1\n",
    "    i += 1\n",
    "\n",
    "############### Load Excel File ###############################################\n",
    "###############################################################################\n",
    "path = os.path.dirname(cwd)+'/Pipeline Monkey Schedule New.xlsx'\n",
    "data = pd.read_excel(path, sheet_name='pico')\n",
    "new_header = data.iloc[0]  \n",
    "data = data[1:]       \n",
    "data.columns = new_header  \n",
    "data = data.fillna('empty')  \n",
    "\n",
    "def add_from_excel(df, data, index):\n",
    "    df.at[ind, 'Has Excel']                 = 1\n",
    "    df.at[ind, '(excel) Index']             = index+2\n",
    "    df.at[ind, '(excel) Stimuli']           = data['Stimuli'][index]\n",
    "    df.at[ind, '(excel) Num Reps']          = data['Reps'][index]\n",
    "    df.at[ind, '(excel) Num Images']        = data['Images*/Videos'][index]\n",
    "    df.at[ind, '(excel) Total Num Images']  = data['Total images/Videos'][index]\n",
    "    df.at[ind, '(excel) Notes']  = data['Notes'][index]\n",
    "\n",
    "\n",
    "for intan, ind in zip(pico_intanRaw_directories, range(len(pico_intanRaw_directories))):\n",
    "    hasexcel = False\n",
    "\n",
    "    ############### Get Recording Info from FileName ##############################\n",
    "    ###############################################################################\n",
    "    date = ((intan.split('/')[11].split('_')[-2]))\n",
    "    time = ((intan.split('/')[11].split('_')[-1]))\n",
    "    name = (intan.split('/')[7])\n",
    "    found_item = False\n",
    "\n",
    "    ############### Filter by Date in FileName and ExcelDate ##########################\n",
    "    ###############################################################################\n",
    "    for exl_date,index in zip(data['Date'], range(1,len(data))):\n",
    "        exl_day, exl_month, exl_year = get_date(exl_date)\n",
    "        \n",
    "        if exl_year+exl_month+exl_day == date:\n",
    "\n",
    "            ############### Filter by FileName and ExcelStimulus ##########################\n",
    "            ###############################################################################\n",
    "            list = [data['Stimuli'][index]]\n",
    "            for i in range(1,10):\n",
    "                try:\n",
    "                    if data['Date'][index+i] == 'empty':\n",
    "                        list.append(data['Stimuli'][index+i])\n",
    "                        i+=1\n",
    "                    else:\n",
    "                        break\n",
    "                except:break\n",
    "\n",
    "            # check stim name\n",
    "            for item, i in zip(list, range(len(list))): \n",
    "                if item.lower() in name:\n",
    "                    add_from_excel(df, data, index+i)\n",
    "                    found_item = True\n",
    "\n",
    "            # check _ and -\n",
    "            if found_item == False:\n",
    "                for item, i in zip(list, range(len(list))):\n",
    "                    name_new = name.lower().replace('-', '_').split('_')\n",
    "                    item_new = item.lower().replace('-', '_').split('_') \n",
    "\n",
    "                    if set(item_new).intersection(set(name_new)) == set(item_new) or set(name_new).intersection(set(item_new)) == set(name_new):\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "\n",
    "            # manually correct rest\n",
    "            if found_item == False:\n",
    "                for item, i in zip(list, range(len(list))):\n",
    "                    if name == 'monkeyvalence' and item.lower() in ['monkeyvalence2','monkeyvalence3', 'monkeyvalence4', \\\n",
    "                                                                    'monkeyvalence5', 'monkeyvalence6', 'monkeyvalence7', 'monkeyvalence8']:\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'shapegen' and intan.split('/')[-1].split('_')[2] == 'static' and item.lower() == 'shapegen - static':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'shapegen' and intan.split('/')[-1].split('_')[2] == 'dynamic' and item.lower() == 'shapegen - dynamic':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == '1_shapes' and item.lower() == 'shapenet images':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'normalizers-HVM' and item.lower() == 'normalizers - hvm':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'facescrub-small' and item.lower() == 'faces_transformation':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'object_relations' and item.lower() == 'object_relationships':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'RF_Mapping_Yoon' and item in ['RF-Yoon Pico version', 'RFMapping_Yoon', 'RF-Yoon Pico version']:\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'emogan' and item.lower() == 'emogen':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "                    if name == 'IAPS' and intan.split('/')[-1].split('_')[2] == '200on' and item == 'IAPS - 200on':\n",
    "                        add_from_excel(df, data, index+i)\n",
    "                        found_item = True\n",
    "\n",
    "    if found_item == False:\n",
    "        print('No Excel: ', date, name, intan)\n",
    "        \n",
    "# display(df.tail(10))\n",
    "\n",
    "############### Save New Excel File ###########################################\n",
    "###############################################################################\n",
    "\n",
    "excel_file_path = f'{os.path.dirname(cwd)}/{inventory_excel_file_name}'  \n",
    "df.to_excel(excel_file_path, index=False) \n",
    "\n",
    "\n",
    "############### Add New Sheet to Excel File ###################################\n",
    "###############################################################################\n",
    "\n",
    "# Create a list to store the results\n",
    "result_list = []\n",
    "\n",
    "# Group the DataFrame by 'ImageSet' and iterate through the groups\n",
    "for group, group_df in df.groupby('ImageSet'):\n",
    "    image_set = group\n",
    "    num_entries = len(group_df)\n",
    "    num_has_excel = group_df['Has Excel'].astype(int).sum()\n",
    "    num_has_spike_time = group_df['Has SpikeTime'].astype(int).sum()\n",
    "    num_has_psth = group_df['Has psth'].astype(int).sum()\n",
    "    num_has_h5 = group_df['Has h5'].astype(int).sum()\n",
    "    \n",
    "    # Append the results to the list\n",
    "    result_list.append({\n",
    "        'ImageSet': image_set,\n",
    "        'Num Rec. Sess.': num_entries,\n",
    "        'Num Has SpikeTime': num_has_spike_time,\n",
    "        'Num Has psth': num_has_psth,\n",
    "        'Num Has h5': num_has_h5,\n",
    "        'Num Has Excel': num_has_excel,\n",
    "        'BrainScore': '',\n",
    "        'Comments': ''\n",
    "    })\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "summary_df = pd.DataFrame(result_list)\n",
    "\n",
    "# Optionally, set 'ImageSet' as the index\n",
    "# summary_df.set_index('ImageSet', inplace=True)\n",
    "summary_df = summary_df.sort_values(by='Num Rec. Sess.', ascending=False)\n",
    "# Display the summary DataFrame\n",
    "display(summary_df.head(5))\n",
    "\n",
    "\n",
    "existing_excel_file = f'{os.path.dirname(cwd)}/{inventory_excel_file_name}' \n",
    "\n",
    "# Create a Pandas ExcelWriter object\n",
    "with pd.ExcelWriter(existing_excel_file, engine='openpyxl', mode='a') as writer:\n",
    "    # Write your DataFrame to a new sheet\n",
    "    summary_df.to_excel(writer, sheet_name='Sheet2', index=False)  # 'Sheet2' is the name of the new sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create/update inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "normalizers\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "muri1320\n",
      "NSD-COCO\n",
      "NSD-COCO\n",
      "NSD-COCO\n",
      "ko_context_size\n",
      "robustness_guy_d1_v40\n",
      "robustness_guy_d1_v40\n",
      "robustness_guy_d1_v40\n",
      "muri1320-2023-v1\n",
      "muri1320-2023-v1\n",
      "muri1320-2023-v1\n",
      "muri1320-2023-v1\n",
      "muri1320-2023-v1\n",
      "muri1320-2023-v1\n",
      "muri1320-2023-v1\n",
      "muri1320-2023-v1\n",
      "HVM-var6-subset-2023\n",
      "images_in_context\n",
      "1_shapes\n",
      "1_shapes\n",
      "1_shapes\n",
      "muri1320-2023-v0\n",
      "muri1320-2023-v0\n",
      "muri1320-2023-v0\n",
      "muri1320-2023-v0\n",
      "muri1320-2023-v0\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "domain-transfer-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "HVM-var6-2023\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "normalizers-HVM\n",
      "facesMSFDE\n",
      "facescrub-small\n",
      "facescrub-small\n",
      "facescrub-small\n",
      "shinecut\n",
      "object_relations\n",
      "object_relations\n",
      "object_relations\n",
      "images_in_context2\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "physion\n",
      "gestalt\n",
      "gestalt\n",
      "gestalt\n",
      "gestalt\n",
      "objectsize\n",
      "oasis900\n",
      "oasis900\n",
      "oasis100c\n",
      "oasis100c\n",
      "oasis100c\n",
      "oasis100c\n",
      "oasis100c\n",
      "oasis100o\n",
      "oasis100o\n",
      "oasis100o\n",
      "oasis100o\n",
      "oasis100o\n",
      "emogan\n",
      "oasis900scrambled\n",
      "oasis900scrambled\n",
      "oasis900scrambled\n",
      "oasis900scrambled\n",
      "oasis900scrambled\n",
      "IAPS\n",
      "IAPS\n",
      "food\n",
      "shapenet360\n",
      "shapenet360\n",
      "shapenet360\n",
      "novel500\n",
      "images_in_context3\n",
      "motionset1\n",
      "IAPS100\n",
      "IAPS100\n",
      "IAPS100\n",
      "IAPS100\n",
      "IAPS100\n",
      "IAPS100c\n",
      "IAPS100c\n",
      "IAPS100c\n",
      "IAPS100c\n",
      "IAPS100c\n",
      "flicker\n",
      "robustness_guy_d0_v24\n",
      "robustness_guy_d0_v24\n",
      "robustness_guy_d0_v24\n",
      "sine_wave\n",
      "sine_wave\n",
      "Alireza_paradigm1\n",
      "Alireza_paradigm2\n",
      "robustness_guy_d0_v26\n",
      "robustness_guy_d1_v26\n",
      "robustness_guy_d1_v27\n",
      "Co3D\n",
      "Co3D\n",
      "Co3D\n",
      "Co3D\n",
      "robustness_guy_d0_v30\n",
      "moca\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "mayo\n",
      "robustness_guy_d0_v31\n",
      "robustness_guy_d1_v31\n",
      "robustness_guy_d0_v32\n",
      "robustness_guy_d1_v32\n",
      "robustness_guy_d1_v33\n",
      "robustness_chong_d0_v7\n",
      "robustness_guy_d0_v7_chong\n",
      "robustness_guy_d1_v7_chong\n",
      "robustness_guy_d1_v7_chong_dryrun\n",
      "robustness_guy_d2_v7_chong\n",
      "robustness_guy_d3_v7_chong\n",
      "robustness_guy_d4_v7_chong\n",
      "robustness_guy_d5_v7_chong\n",
      "robustness_guy_d6_v7_chong\n",
      "robustness_guy_d7_v7_chong\n",
      "gratingsAdap_s1\n",
      "gratingsAdap_s2\n",
      "gratingsAdap_s3\n",
      "gratingsAdap_s4\n",
      "gratingsAdap_s5\n",
      "square_sinewave\n",
      "faceemovids\n",
      "faceemovids\n",
      "oasis900rotated\n",
      "oasis900rotated\n",
      "robustness_guy_d0_v34\n",
      "robustness_guy_d1_v34\n",
      "robustness_guy_d1_v34\n",
      "robustness_guy_d0_v35\n",
      "robustness_guy_d1_v35\n",
      "robustness_guy_d1_v35\n",
      "robustness_guy_d1_v36\n",
      "robustness_guy_d0_v37\n",
      "robustness_guy_d1_v37\n",
      "robustness_guy_d1_v39\n",
      "robustness_guy_d1_v39\n",
      "afv\n",
      "Kar_2023_degraded\n",
      "robustness_guy_d0_v40\n",
      "robustness_guy_d1_v41\n",
      "robustness_guy_d1_v41\n",
      "IAPS-200on\n",
      "IAPS-200on\n",
      "shapegen_static\n",
      "shapegen_static\n",
      "shapegen_dynamic\n",
      "shapegen_dynamic\n",
      "oasis900_200on\n",
      "monkeyvalence-200on\n",
      "monkeyvalence2\n",
      "monkeyvalence2-200on\n",
      "monkeyvalence3\n",
      "monkeyvalence4\n",
      "monkeyvalence6\n",
      "monkeyvalence7\n",
      "monkeyvalence8\n",
      "oasis900scrambled_200on\n",
      "oasis900scrambled_200on\n"
     ]
    }
   ],
   "source": [
    "############### Create Directories ############################################\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( f'{os.path.dirname(cwd)}/{inventory_excel_file_name}'  )\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory_new'\n",
    "storage_old = '/braintree/home/aliya277/inventory'\n",
    "\n",
    "for index, DataFrame in df.iterrows():\n",
    "        \n",
    "    if DataFrame['Has SpikeTime'] == 1:\n",
    "        date = f\"20{DataFrame['date']}\"\n",
    "        if len(str(DataFrame['time'])) != 6: time = f\"0{DataFrame['time']}\"\n",
    "        else: time = str(DataFrame['time'])\n",
    "        \n",
    "        if DataFrame['ImageSet'] == 'normalizers':\n",
    "            directory = f'norm_FOSS.sub_pico.{date}_{time}.proc'\n",
    "        elif DataFrame['ImageSet'] == 'normalizers-HVM':\n",
    "            directory = f'norm_HVM.sub_pico.{date}_{time}.proc'\n",
    "        else: \n",
    "            directory = f\"exp_{DataFrame['ImageSet']}.sub_pico.{date}_{time}.proc\"\n",
    "\n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "        subjectdir_date  = os.path.join(subjectdir, \".\".join(directory.split(\".\")[0:2])+'.'+date)\n",
    "        print(DataFrame['ImageSet'])\n",
    "        try: os.mkdir(imagesetdir)\n",
    "        except: pass\n",
    "        try: os.mkdir(subjectdir)\n",
    "        except: pass\n",
    "        try: os.mkdir(subjectdir_date, directory)\n",
    "        except: pass\n",
    "            \n",
    "        if not os.path.isdir(os.path.join(subjectdir_date, directory, 'SpikeTimes')):\n",
    "            print(f'Copying SpikeTimes for {directory}')\n",
    "            shutil.copytree(DataFrame['Path: SpikeTimes'], os.path.join(subjectdir_date, directory, 'SpikeTimes'))\n",
    "            \n",
    "        if not os.path.isdir(os.path.join(subjectdir_date, directory, 'psth')) and DataFrame['Has psth'] == 1: \n",
    "            print(f'Copying psth for {directory}')\n",
    "            os.mkdir(os.path.join(subjectdir_date, directory, 'psth'))\n",
    "            psth_path = DataFrame['Path: psth']\n",
    "            n_channels = len(os.listdir(DataFrame['Path: SpikeTimes']))\n",
    "            path_intanproc = os.path.join('/', *psth_path.split('/')[:-1])\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # copy psth from intanproc path to inventory\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            combined = False\n",
    "            for ele in os.listdir(path_intanproc):\n",
    "                if ele.endswith(\"_psth.mat\"): \n",
    "                    combined = True\n",
    "                    shutil.copy2(path_intanproc+'/'+ele, os.path.join(subjectdir_date, directory, 'psth', ele))\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # combine channels and save in inventory\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            if not combined: \n",
    "                n_channels = len(os.listdir(path_intanproc+'/psth'))\n",
    "                combine_channels(psth_path, os.path.join(subjectdir_date, directory, 'psth'), n_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Add Sarah's BrainScore Column #################################\n",
    "###############################################################################\n",
    "\n",
    "df = pd.read_excel( f'{os.path.dirname(cwd)}/{inventory_excel_file_name}' , sheet_name='Sheet2').sort_values(by='ImageSet')\n",
    "df_sarah = pd.read_excel( f'{os.path.dirname(cwd)}/pico_inventory_sarah.xlsx', sheet_name='Sheet2').sort_values(by='ImageSet')\n",
    "\n",
    "for ind, row in df.iterrows():\n",
    "    if not row['ImageSet'] in df_sarah['ImageSet'].tolist(): print(f\"New ImageSets: {row['ImageSet']}\")\n",
    "    index = np.where(df_sarah[\"ImageSet\"].to_numpy() == row['ImageSet'])[0]\n",
    "    if len(df_sarah.iloc[index]['BrainScore'].tolist()) > 0: df.at[ind,\"BrainScore\"] = df_sarah.iloc[index]['BrainScore'].tolist()[0]\n",
    "    if len(df_sarah.iloc[index]['Comments'].tolist()) > 0:   df.at[ind,\"Comments\"] = df_sarah.iloc[index]['Comments'].tolist()[0]\n",
    "\n",
    "# Update Sheet 2\n",
    "xls = pd.ExcelFile(f'{os.path.dirname(cwd)}/{inventory_excel_file_name}')\n",
    "sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "\n",
    "sheets['Sheet2'] = df  \n",
    "\n",
    "with pd.ExcelWriter(f'{os.path.dirname(cwd)}/{inventory_excel_file_name}', engine='openpyxl', mode='w') as writer:\n",
    "    for sheet_name, sheet_df in sheets.items():\n",
    "        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ \n",
    "# Delete all faulty normalizer files.\n",
    "# ------------------------------------------------------------------------------ \n",
    "\n",
    "experiment_file_paths = glob.glob(os.path.join(root_dir, '*', '*', '*', '*'))\n",
    "\n",
    "from scipy.io import loadmat\n",
    "import shutil\n",
    "\n",
    "for experiment_path in experiment_file_paths: \n",
    "    experiment_name =  \"_\".join(os.path.basename(experiment_path).split('.')[0].split('_')[1:])\n",
    "    for file in os.listdir(experiment_path):\n",
    "        if file == 'psth': \n",
    "            path = os.path.join(experiment_path, 'psth', os.listdir(os.path.join(experiment_path, file))[0])\n",
    "            mat = loadmat(path)\n",
    "            if mat['psth'].shape[-1] != 288 and mat['psth'].shape[-1] != 192:\n",
    "                print(mat['psth'].shape)\n",
    "                shutil.rmtree(os.path.join(experiment_path, 'psth'))\n",
    "            del mat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Create inventory from path (only for new data not from sarah's pico data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190220_143521.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190220_160047.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190221_095435.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190222_120151.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190225_111537.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190226_094812.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190227_122653.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190304_120611.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190305_112242.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190306_122612.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190307_132706.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190308_120834.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190311_113347.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190312_130545.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190317_115104.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190318_114005.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190319_115956.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190322_114704.proc\n",
      "Copying SpikeTimes for exp_bold5000.sub_solo.20190326_100317.proc\n"
     ]
    }
   ],
   "source": [
    "############### Create Directories ############################################\n",
    "###############################################################################\n",
    "\n",
    "storage_dir = '/braintree/home/aliya277/inventory_new'\n",
    "\n",
    "experiment_path = '/braintree/home/aliya277/sachis_data/bold5000/intanproc'\n",
    "# experiment_path = '/braintree/home/aliya277/sachis_data/nat300/intanproc'\n",
    "# experiment_path = '/braintree/home/aliya277/sachis_data/hvm/intanproc'\n",
    "\n",
    "experiment_names = os.listdir(experiment_path)\n",
    "\n",
    "for experiment in experiment_names:\n",
    "    if not experiment.startswith('.'):\n",
    "        subj        = experiment.split('_')[0]\n",
    "        imageset    = experiment.split('_')[1]\n",
    "        date        = experiment.split('_')[-2]\n",
    "        time        = experiment.split('_')[-1]\n",
    "\n",
    "        if len(date) != 8: date = f'20{date}'\n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Create directories in inventory.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "\n",
    "        directory = f\"exp_{imageset}.sub_{subj}.{date}_{time}.proc\"\n",
    "\n",
    "        imagesetdir = os.path.join(storage_dir, \".\".join(directory.split(\".\")[0:1]))\n",
    "        subjectdir  = os.path.join(storage_dir, imagesetdir, \".\".join(directory.split(\".\")[0:2]))\n",
    "        subjectdir_date  = os.path.join(subjectdir, \".\".join(directory.split(\".\")[0:2])+'.'+date)\n",
    "        try: os.mkdir(imagesetdir)\n",
    "        except: pass\n",
    "        try: os.mkdir(subjectdir)\n",
    "        except: pass\n",
    "        try: os.mkdir(subjectdir_date)\n",
    "        except: pass\n",
    "        try: os.mkdir(os.path.join(subjectdir_date, directory))\n",
    "        except: pass\n",
    "        \n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Copy SpikeTimes and PSTH in inventory.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "\n",
    "        path_intanproc = os.path.join(experiment_path, experiment)\n",
    "        if os.path.isdir(path_intanproc):\n",
    "            for ele in os.listdir(path_intanproc):\n",
    "                if ele.endswith(\"_psth.normalized.mat\") and not ele.startswith(\".\") and not os.path.isdir(os.path.join(subjectdir_date, directory, 'psth_normalized')): \n",
    "                    print(f'Copying normalized psth for {directory}')\n",
    "                    try:  os.mkdir(os.path.join(subjectdir_date, directory, 'psth_normalized'))\n",
    "                    except: pass\n",
    "                    shutil.copy2(path_intanproc+'/'+ele, os.path.join(subjectdir_date, directory, 'psth_normalized', ele))\n",
    "\n",
    "            if not os.path.isdir(os.path.join(subjectdir_date, directory, 'SpikeTimes')):\n",
    "                print(f'Copying SpikeTimes for {directory}')\n",
    "                shutil.copytree(os.path.join(path_intanproc,'spikeTime'), os.path.join(subjectdir_date, directory, 'SpikeTimes'))\n",
    "\n",
    "\n",
    "            if not os.path.isdir(os.path.join(subjectdir_date, directory, 'psth')):\n",
    "                print(f'Copying psth for {directory}')\n",
    "                psth_path = os.path.join(path_intanproc,'psth')\n",
    "                n_channels = len(os.listdir(psth_path))\n",
    "                os.mkdir(os.path.join(subjectdir_date, directory, 'psth'))\n",
    "                # print(n_channels)\n",
    "                combine_channels(psth_path, os.path.join(subjectdir_date, directory, 'psth'), n_channels)\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
