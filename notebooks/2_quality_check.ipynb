{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ \n",
    "# Define parameters:\n",
    "# ------------------------------------------------------------------------------ \n",
    "n_boot=10000\n",
    "integration_start_ms    = 70\n",
    "integration_end_ms      = 170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from brainio.assemblies import NeuronRecordingAssembly\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "import glob, os\n",
    "from datetime import datetime\n",
    "import pytz  # This is required to handle timezone conversions\n",
    "import sys\n",
    "import io as ioprint\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "from ndashboard.nquality.raw_data_template import SessionNeuralData\n",
    "from ndashboard.nquality.quality_within_session import Session\n",
    "from ndashboard.nquality.quality_across_sessions import LongitudinalQuality\n",
    "\n",
    "def get_unix_timestamp(date_str, time_str, date_format='%Y%m%d', time_format='%H%M%S'):\n",
    "    datetime_str = f\"{date_str} {time_str}\"\n",
    "    dt = datetime.strptime(datetime_str, f\"{date_format} {time_format}\")\n",
    "    # Assuming the provided time is in UTC\n",
    "    # If it's in another timezone, you can adjust it accordingly using pytz\n",
    "    dt = pytz.utc.localize(dt)\n",
    "    unix_timestamp = (dt.timestamp())\n",
    "    return unix_timestamp\n",
    "\n",
    "def generate_timestamps(start_timestamp, interval_ms, length):\n",
    "    # Create an array of increments (100ms steps)\n",
    "    increments = np.arange(0, length * interval_ms / 1000, interval_ms / 1000)\n",
    "    timestamps = start_timestamp + increments\n",
    "    return timestamps\n",
    "\n",
    "def create_norm_assembly(psth, meta, start_timestemp = None):\n",
    "    timebase = np.arange(meta[0], meta[1], meta[2])\n",
    "    timebins = np.asarray([[int(x), int(x)+int(meta[2])] for x in timebase])\n",
    "    assert len(timebase) == psth.shape[2], f\"Number of bins is not correct. Expected {len(timebase)} got {psth.shape[2]}\"\n",
    "    \n",
    "    assembly = xr.DataArray(psth,\n",
    "                    coords={'repetition': ('repetition', list(range(psth.shape[1]))),\n",
    "                            'stimulus_id': ('image', list(range(psth.shape[0]))),\n",
    "                            'time_bin_id': ('time_bin', list(range(psth.shape[2]))),\n",
    "                            'time_bin_start': ('time_bin', [x[0] for x in timebins]),\n",
    "                            'time_bin_stop': ('time_bin', [x[1] for x in timebins])},\n",
    "                    dims=['image', 'repetition', 'time_bin', 'neuroid'])\n",
    "\n",
    "    assembly = assembly.stack(presentation=('image', 'repetition')).reset_index('presentation')\n",
    "    assembly = assembly.drop('image')\n",
    "    assembly = assembly.isel(time_bin = slice(int(0-(meta[0]/meta[2])+(integration_start_ms/meta[2])), int(0-(meta[0]/meta[2])+(integration_end_ms/meta[2])))).sum('time_bin').transpose('presentation', 'neuroid')\n",
    "    \n",
    "    if start_timestemp == None: \n",
    "        assembly = assembly.assign_coords({'unix_timestamp': ('presentation', np.linspace(0, integration_end_ms-integration_start_ms, assembly.shape[0]))})\n",
    "    else: \n",
    "        timestamps = generate_timestamps(start_timestemp, interval_ms=integration_end_ms-integration_start_ms, length=assembly.shape[0])\n",
    "        assembly   = assembly.assign_coords({'unix_timestamp': ('presentation', timestamps)})\n",
    "    numchannels = assembly.shape[1]\n",
    "\n",
    "    return assembly, numchannels\n",
    "\n",
    "def find_norm_with_date(realdate):\n",
    "    all_paths = []\n",
    "    normalizer_file_paths = glob.glob(os.path.join(root_dir, '[norm]*', '*', '*', '[!h5]*'))\n",
    "    for norm_file_path in normalizer_file_paths:\n",
    "        date, time = os.path.basename(norm_file_path).split('.')[-2].split('_')\n",
    "        if date == realdate: \n",
    "            all_paths.append(norm_file_path)\n",
    "\n",
    "    if len(all_paths) == 0: return [None]   \n",
    "    \n",
    "    return all_paths\n",
    "\n",
    "def get_list_of_days(days):\n",
    "    list_days = []\n",
    "    for day in days:\n",
    "        list_days.append(day.split('.')[-1])\n",
    "    return list_days\n",
    "\n",
    "def quality_within_session(day):\n",
    "    \n",
    "    norm_paths = find_norm_with_date(day)\n",
    "    for norm_path in norm_paths:\n",
    "        if norm_path != None:\n",
    "            print(norm_path)\n",
    "            print(glob.glob(os.path.join(norm_path, '*[nwb]')))\n",
    "            norm_nwb_file_path = glob.glob(os.path.join(norm_path, '*[nwb]'))[0]\n",
    "            \n",
    "            io = NWBHDF5IO(norm_nwb_file_path, \"r\") \n",
    "            norm_nwbfile = io.read()\n",
    "            try: \n",
    "                psth = norm_nwbfile.scratch['psth'][:]\n",
    "                meta = norm_nwbfile.scratch['psth meta'][:]\n",
    "                #print(psth.shape)\n",
    "                assert psth.shape[0] == 86 or psth.shape[0] == 26 # norm FOSS has 86 and norm HVM has 26 images\n",
    "\n",
    "                print(f\"Within: using normalizer file {os.path.basename(norm_nwb_file_path)}\")\n",
    "                print(psth.shape, meta)\n",
    "                io.close()\n",
    "                da, nc  = create_norm_assembly(psth, meta)\n",
    "                session = SessionNeuralData(da)\n",
    "                session = Session(session, boot_seed=0, nboot=n_boot)\n",
    "                ds_quality = session.ds_quality\n",
    "                pvalues = ds_quality['pvalue_signal_variance'].data\n",
    "                print('P Values of first session: ', pvalues)\n",
    "                \n",
    "                return pvalues\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                print(f'No psth available for normalizer {day}.')\n",
    "                return [None]\n",
    "        else: \n",
    "            print(f'No normalizer found for day {day}.')\n",
    "            return [None]\n",
    "\n",
    "def quality_across_sessions(first_day, comparing_day):\n",
    "    \n",
    "    norm_paths_1 = find_norm_with_date(first_day)\n",
    "    norm_paths_2 = find_norm_with_date(comparing_day)\n",
    "\n",
    "    if len(norm_paths_2) == 2: norm_paths_1.append(norm_paths_1[0])\n",
    "\n",
    "    for norm_path_1, norm_path_2 in zip(norm_paths_1, norm_paths_2):\n",
    "        if norm_path_1 != None and norm_path_2 != None:\n",
    "\n",
    "            norm_nwb_file_path_1 = glob.glob(os.path.join(norm_path_1, '*[nwb]'))[0]\n",
    "            norm_nwb_file_path_2 = glob.glob(os.path.join(norm_path_2, '*[nwb]'))[0]\n",
    "\n",
    "            try: \n",
    "                io = NWBHDF5IO(norm_nwb_file_path_1, \"r\") \n",
    "                norm_nwbfile = io.read()\n",
    "                psth_1 = norm_nwbfile.scratch['psth'][:]\n",
    "                meta_1 = norm_nwbfile.scratch['psth meta'][:]\n",
    "                io.close()\n",
    "                \n",
    "                io = NWBHDF5IO(norm_nwb_file_path_2, \"r\") \n",
    "                norm_nwbfile = io.read()\n",
    "                psth_2 = norm_nwbfile.scratch['psth'][:]\n",
    "                meta_2 = norm_nwbfile.scratch['psth meta'][:]\n",
    "                io.close()\n",
    "\n",
    "                assert psth_1.shape[0] == 86 or psth_1.shape[0] == 26 # norm FOSS has 86 and norm HVM has 26 images\n",
    "                assert psth_2.shape[0] == 86 or psth_2.shape[0] == 26\n",
    "                assert psth_1.shape[0] == psth_2.shape[0] # only compare from the same normalizer image set\n",
    "\n",
    "                print(f\"Across: using normalizer file {os.path.basename(norm_nwb_file_path_1)} and {os.path.basename(norm_nwb_file_path_2)} \")\n",
    "                print(psth_1.shape, meta_1, psth_2.shape, meta_2)\n",
    "                \n",
    "                n_channel_1 = psth_1.shape[-1]\n",
    "                da, nc  = create_norm_assembly(psth_1, meta_1)\n",
    "                session_1 = SessionNeuralData(da)\n",
    "\n",
    "                n_channel_2 = psth_2.shape[-1]\n",
    "                da, nc  = create_norm_assembly(psth_2, meta_2)\n",
    "                session_2 = SessionNeuralData(da)\n",
    "\n",
    "                if n_channel_1 == n_channel_2:\n",
    "                    session = LongitudinalQuality([session_1, session_2], boot_seed=0, nboot=n_boot)\n",
    "                    ds_quality = session.ds_quality\n",
    "                    pvalues = ds_quality['pvalue_signal_variance'].data\n",
    "                    return pvalues\n",
    "                else: \n",
    "                    print(f\"Number of channels do not match. {n_channel_1} != {n_channel_2} \")\n",
    "                    return [None, None]\n",
    "                \n",
    "            except Exception as error:\n",
    "                print(error)\n",
    "                print(f'No psth available for normalizers {first_day, comparing_day}.')\n",
    "                return [None, None]\n",
    "            \n",
    "        else: \n",
    "            print(f'No normalizer found for day {first_day, comparing_day}.')\n",
    "            return [None, None]\n",
    "\n",
    "def update_sheet(df, exp_nwb_path, text):\n",
    "    imageset = os.path.basename(exp_nwb_path).split('.')[0].split('_')[1:]\n",
    "    if len(imageset) == 1: imageset = imageset[0]\n",
    "    elif len(imageset) > 1: imageset = '_'.join(imageset)\n",
    "    mask = df['ImageSet'] == imageset\n",
    "    index = df.index[mask].tolist()[0]\n",
    "    df.at[index, 'proc_nwb'] = text\n",
    "        \n",
    "root_dir        = '/braintree/home/aliya277/inventory_new'\n",
    "\n",
    "df = pd.read_excel( os.path.dirname(cwd)+'/pico_inventory.xlsx' , sheet_name='Sheet2')\n",
    "df['proc_nwb'] = 'No QC.'\n",
    "# list of experiments sarah marked as 'not going on brainscore'\n",
    "list_exp_not_using = [row['ImageSet'] for index, row in df.iterrows() if row['BrainScore'] != 'Y']\n",
    "\n",
    "SubjectName = 'pico'\n",
    "storage_dir = '/braintree/home/aliya277/inventory_new'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: For each experiment, do a within-session and across-session quality check and save them in the experiment nwb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_file_paths = glob.glob(os.path.join(root_dir, '[exp]*', '*'))\n",
    "experiment_file_paths = [d for d in experiment_file_paths if 'VideoStimulusSet' not in os.path.basename(d)]\n",
    "\n",
    "\n",
    "for experiment_path in experiment_file_paths: \n",
    "\n",
    "    days    = glob.glob(os.path.join(experiment_path, '*[!npy][!txt][!nwb]'))\n",
    "    n_days  = len(days)\n",
    "    n_sessions = 0\n",
    "    for day in days :\n",
    "        n_sessions += len(glob.glob(os.path.join(experiment_path, day, '*', '*proc.nwb'))) # TO DO: REMOVE OLD\n",
    "    first_day = days[0].split('.')[-1]\n",
    "\n",
    "    \n",
    "    #if not os.path.basename(experiment_path) == 'exp_hvm.sub_solo': continue \n",
    "    # if not os.path.basename(experiment_path) == 'exp_nat300.sub_solo': continue \n",
    "    # if not os.path.basename(experiment_path) == 'exp_bold5000.sub_solo': continue \n",
    "\n",
    "    print('________________________________________________________________________________')\n",
    "    print(f'{os.path.basename(experiment_path)} has {n_days} days and {n_sessions} sessions')\n",
    "    \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Skip code, if experiment is not wanted (see list above) or if nwb not exists.\n",
    "    # ------------------------------------------------------------------------------  \n",
    "\n",
    "    if os.path.basename(experiment_path).split(\"_\")[1].split('.')[0] in list_exp_not_using: \n",
    "        print(os.path.basename(experiment_path).split(\"_\")[1].split('.')[0])\n",
    "        update_sheet(df, experiment_path, 'Experiment is not going on BrainScore')\n",
    "        continue \n",
    "    \n",
    "    if n_sessions == 0: \n",
    "        print(f'{os.path.basename(experiment_path)} has no nwb file.')\n",
    "        update_sheet(df, experiment_path, 'No nwb files in experiment.')\n",
    "        continue     \n",
    "    \n",
    "    if os.path.isfile(os.path.join(experiment_path, 'pvalues_first_day.npy')):\n",
    "        pvalues_first_day = np.load(os.path.join(experiment_path, 'pvalues_first_day.npy'))\n",
    "\n",
    "    else:\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Do within Session QC for the sessions of the first day.  \n",
    "        # ------------------------------------------------------------------------------  \n",
    "        pvalues_first_day = quality_within_session(first_day)\n",
    "\n",
    "        if pvalues_first_day[0] == None: \n",
    "            update_sheet(df, experiment_path, 'No normalizers.')\n",
    "            continue \n",
    "        np.save(os.path.join(experiment_path, 'pvalues_first_day.npy'), pvalues_first_day)\n",
    "    \n",
    "    if n_days > 1: \n",
    "        run_again = True\n",
    "        if os.path.isfile(os.path.join(experiment_path, 'corss_session_pvalues.npy')): \n",
    "            run_again = False\n",
    "            corss_session_pvalues = np.load(os.path.join(experiment_path, 'corss_session_pvalues.npy'))\n",
    "            if len(corss_session_pvalues) != n_days-1: run_again = True\n",
    "\n",
    "        if run_again==True:\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # Do across session QC for the other sessions with the first day. \n",
    "            # ------------------------------------------------------------------------------  \n",
    "            list_all_days = get_list_of_days(days[1:])\n",
    "            corss_session_pvalues = []\n",
    "            for comparing_day in list_all_days:\n",
    "                p_values = quality_across_sessions(first_day, comparing_day)\n",
    "                corss_session_pvalues.append(p_values)\n",
    "            \n",
    "            if any(element is None for sublist in corss_session_pvalues for element in sublist): \n",
    "                print(\"Did not find all the normalizers.\")\n",
    "                update_sheet(df, experiment_path, 'No normalizers.')\n",
    "                continue\n",
    "            else:\n",
    "                if np.array(corss_session_pvalues == None).sum() == 0:\n",
    "                    for val in corss_session_pvalues: print(np.allclose(np.where(pvalues_first_day<0.05), np.where(val[0,:]<0.05)))\n",
    "\n",
    "                np.save(os.path.join(experiment_path, 'corss_session_pvalues.npy'), corss_session_pvalues)\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Add p-values to respective experiment nwb files.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "\n",
    "    for day, i_day in zip(days, range(n_days)) :\n",
    "        print(f'Checking if P-Values are added to experiment file for day {i_day+1}')\n",
    "        exp_nwb_paths = (glob.glob(os.path.join(experiment_path, day, '*',  '*proc.nwb')))\n",
    "\n",
    "        if i_day ==0:   current_pvalues = pvalues_first_day\n",
    "        else:           current_pvalues = corss_session_pvalues[i_day-1][1,:]\n",
    "\n",
    "        if current_pvalues[0] == None: continue\n",
    "\n",
    "        for exp_nwb_path in exp_nwb_paths:\n",
    "            \n",
    "            print(i_day, os.path.basename(exp_nwb_path))\n",
    "            io = NWBHDF5IO(exp_nwb_path, \"a\") \n",
    "            exp_nwbfile = io.read()\n",
    "            try:\n",
    "                n_channel = exp_nwbfile.scratch['psth'][:].shape[-1]\n",
    "                n_channel_norm = pvalues_first_day.shape[0]\n",
    "                assert n_channel == n_channel_norm\n",
    "            except Exception as e: print(e, 'Check Channel numbers for normalizers and experiment.')\n",
    "\n",
    "            try: \n",
    "                exp_nwbfile.scratch['PValuesPerChannel'][:]\n",
    "                print('     P-Values are already added.')\n",
    "            except:\n",
    "                print(\"Adding pvalues to nwb.\")\n",
    "                exp_nwbfile.add_scratch(\n",
    "                        current_pvalues,\n",
    "                        name=\"PValuesPerChannel\",\n",
    "                        description=f\"An array of length equal to the number of electrodes, where each entry corresponds to \\\n",
    "                            the p-value for the electrode with the matching ID. For the initial recording, a within-session \\\n",
    "                            quality check is conducted; for subsequent recordings, a cross-session quality comparison is \\\n",
    "                            performed with the initial recording. Parameters used: boot_seed=0, nboot={n_boot}.\",\n",
    "                        )\n",
    "                        \n",
    "                io.write(exp_nwbfile)\n",
    "            io.close()\n",
    "    update_sheet(df, experiment_path, 'P-Values added.')\n",
    "                \n",
    "# Update Sheet 2\n",
    "xls = pd.ExcelFile(f'{os.path.dirname(cwd)}/pico_inventory.xlsx')\n",
    "sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "sheets['Sheet2'] = df  \n",
    "with pd.ExcelWriter(f'{os.path.dirname(cwd)}/pico_inventory.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "    for sheet_name, sheet_df in sheets.items():\n",
    "        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
