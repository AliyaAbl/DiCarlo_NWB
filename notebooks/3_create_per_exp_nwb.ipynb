{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ \n",
    "# Define parameters:\n",
    "# ------------------------------------------------------------------------------ \n",
    "p_value_threshold       = 0.05\n",
    "integration_start_ms    = 70\n",
    "integration_end_ms      = 170\n",
    "test_train_split        = 0.85 # percentage of the train split, i.e. test split size is 1 - this.\n",
    "min_QC_channel_tresh    = 0.10 # percentage of channels, which need to be good to keep this session.\n",
    "min_number_QC_channel_tresh = 15 # minimum number of QC channels required to merge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import h5py\n",
    "from brainio.assemblies import NeuronRecordingAssembly\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "import glob, os, yaml\n",
    "import pytz  # This is required to handle timezone conversions\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os, glob, json\n",
    "import pandas as pd\n",
    "from pynwb.file import Subject\n",
    "import logging, sys, shutil\n",
    "import re\n",
    "\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "root_dir        = '/braintree/home/aliya277/inventory_new'\n",
    "\n",
    "def read_names(filename):\n",
    "    assignment  = filename.split('.')[0].split('-')[1]\n",
    "    number      = filename.split('.')[0].split('-')[2]\n",
    "    return np.asarray([assignment, number])\n",
    "\n",
    "def create_prom_nwb(config, path, experiment_name):\n",
    "\n",
    "    desired_timezone = pytz.timezone('US/Eastern')\n",
    "    pattern = r'Fixation\\s*:\\s*(\\d+), Visual\\s*:\\s*(\\d+), ON/OFF\\s*:\\s*(\\d+/\\d+)'\n",
    "    matches = re.search(pattern, config['session_info']['session_description'])\n",
    "    if matches: notes_experiment = matches.group(0)\n",
    "    else: \n",
    "        pattern = r'(Fixation|Visual|ON/OFF)\\s*:\\s*(\\d+|\\d+-\\d+|\\d+/\\d+)'\n",
    "        matches = re.findall(pattern, config['session_info']['session_description'])\n",
    "        notes_experiment = ', '.join(f'{key}: {value}' for key, value in matches)\n",
    "\n",
    "    ################ CREATE NWB FILE WITH METADATA ################################\n",
    "    ###############################################################################\n",
    "    nwbfile = NWBFile(\n",
    "        session_description     = 'This NWB is derived from the processing of neurophysiological recordings in the inferior temporal (IT)\\\n",
    "        cortex of one macaque monkey (indicated in the file). These recordings were conducted during Rapid Serial Visual Presentation (RSVP) of \\\n",
    "        randomized images, each presented at the center of gaze during fixation. The dataset in this file includes a responses from a number of \\\n",
    "        individual neural recording sites collected via chronically implanted Utah arrays. For each recording site, the multiunit or single unit \\\n",
    "        response of that site is summarized as a set of peristimulus time histograms (PSTH). Each PSTH is derived from the siteâ€™s response to repeated, \\\n",
    "        emporally randomized, presentations of the same image. Thus, the number of PSTHs is identical to the number of images in the stimulus set \\\n",
    "        (typically 100-2000 images). The number of repetitions of each image is indicated in the file and the data from individual repetitions are \\\n",
    "        available in the file. For larger image sets these recordings were made over a series of consecutive days (usually <5 days). Quality control \\\n",
    "        measures (e.g. consistency of response pattern over images) collected on each day are used to check for site stability across those days \\\n",
    "        before producing the final PSTH estimates for each site. The corresponding visual images used in these recordings are linked or stored in \\\n",
    "        the \"Stimulus_Template\" section of each NWB file.',\n",
    "        identifier              = str(uuid4()),\n",
    "        session_start_time      = desired_timezone.localize(config['metadata']['session_start_time']),\n",
    "        file_create_date        = datetime.now(desired_timezone), #desired_timezone.localize(config['metadata']['file_create_date']),\n",
    "        experimenter            = config['general']['lab_info']['experimenter'],\n",
    "        experiment_description  = config['general']['experiment_info']['experiment_description'],\n",
    "        session_id              = experiment_name, #config['session_info']['session_id'],\n",
    "        lab                     = config['general']['lab_info']['lab'],                     \n",
    "        institution             = config['general']['lab_info']['university'],                                    \n",
    "        keywords                = config['general']['experiment_info']['keywords'],\n",
    "        surgery                 = config['general']['experiment_info']['surgery'],\n",
    "        notes                   = notes_experiment\n",
    "    )\n",
    "\n",
    "    ################ CREATE SUBJECT ################################################\n",
    "    ################################################################################\n",
    "    nwbfile.subject = Subject(\n",
    "        subject_id  = config['subject']['subject_id'],\n",
    "        date_of_birth= config['subject']['date_of_birth'],\n",
    "        species     = config['subject']['species'],\n",
    "        sex         = config['subject']['sex'],\n",
    "        description = config['subject']['description'],\n",
    "    )\n",
    "\n",
    "    ################ CREATE HARDWARE LINKS #########################################\n",
    "    ################################################################################\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['system_name'], \n",
    "        description = config['hardware']['system_description'], \n",
    "        manufacturer= config['hardware']['system_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['adapter_manuf'], \n",
    "        description = config['hardware']['adapter_description'], \n",
    "        manufacturer= config['hardware']['adapter_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['monitor_name'], \n",
    "        description = config['hardware']['monitor_description'], \n",
    "        manufacturer= config['hardware']['monitor_manuf']\n",
    "    )\n",
    "\n",
    "    nwbfile.create_device(\n",
    "        name        = config['hardware']['photodiode_name'], \n",
    "        description = config['hardware']['photodiode_description'], \n",
    "        manufacturer= config['hardware']['photodiode_manuf']\n",
    "    )\n",
    "    \n",
    "    nwbfile.create_device(\n",
    "        name        = 'Software Used', \n",
    "        description = str(['Mworks Client: '+config['software']['mwclient_version'],\\\n",
    "                        'Mworks Server: '+config['software']['mwserver_version'],\\\n",
    "                        'OS: '+config['software']['OS'],\\\n",
    "                        'Intan :'+config['software']['intan_version']])\n",
    "    )\n",
    "\n",
    "    ################ CREATE ELECTRODE LINKS ########################################\n",
    "    ################################################################################\n",
    "    electrodes = nwbfile.create_device(\n",
    "        name        = config['hardware']['electrode_name'], \n",
    "        description = config['hardware']['electrode_description'], \n",
    "        manufacturer= config['hardware']['electrode_manuf']\n",
    "    )\n",
    "    if config['subject']['subject_id'] != 'solo':\n",
    "        all_files = sorted(os.listdir(os.path.join(path, 'SpikeTimes')))\n",
    "        \n",
    "        name_accumulator = []\n",
    "        for file in all_files:\n",
    "            name_accumulator.append(read_names(file))\n",
    "        names = np.vstack(name_accumulator)\n",
    "\n",
    "    elif config['subject']['subject_id'] == 'solo':\n",
    "        with open('/braintree/home/aliya277/sachis_data/solo_mapping.json', 'r') as file:\n",
    "            data = json.load(file)\n",
    "        list_values = data['neuroid_id'].values()\n",
    "        names_list = [value.split('-') for value in list_values]\n",
    "        list_assignment_number_int = [[pair[0], int(pair[1])] for pair in names_list]\n",
    "\n",
    "        # Convert the list to a numpy array\n",
    "        names = np.array(list_assignment_number_int, dtype='object')\n",
    "\n",
    "\n",
    "    nwbfile.add_electrode_column(name=\"label\", description=\"label of electrode\")\n",
    "    groups, count_groups = np.unique(names[:,0], return_counts =True)\n",
    "    ids                  = names[:,1]\n",
    "    counter              = 0\n",
    "    # create ElectrodeGroups A, B, C, ..\n",
    "    for group, count_group in zip(groups, count_groups):\n",
    "        if len(groups) == 6:\n",
    "            electrode_description = \"Serialnumber: {}. Adapter Version: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber'],\\\n",
    "                            config['array_info']['array_{}'.format(group)]['adapterversion']),\n",
    "        else: \n",
    "            electrode_description = \"Serialnumber: {}\".format(config['array_info']['array_{}'.format(group)]['serialnumber']),\n",
    "                \n",
    "        \n",
    "        electrode_group = nwbfile.create_electrode_group(\n",
    "            name        = \"group_{}\".format(group),\n",
    "            description = electrode_description[0],\n",
    "            device      = electrodes,\n",
    "            location    = 'hemisphere, region, subregion: '+str([config['array_info']['array_{}'.format(group)]['hemisphere'],\\\n",
    "                                config['array_info']['array_{}'.format(group)]['region'],\n",
    "                                config['array_info']['array_{}'.format(group)]['subregion']]),\n",
    "            position    = config['array_info']['array_{}'.format(group)]['position']\n",
    "        )\n",
    "\n",
    "        # create Electrodes 001, 002, ..., 032 in ElectrodeGroups per channel\n",
    "        for ichannel in range(count_group):\n",
    "            nwbfile.add_electrode(\n",
    "                group       = electrode_group,\n",
    "                label       = ids[counter],\n",
    "                location    = 'row, col, elec'+str(json.loads(config['array_info']['intan_electrode_labeling_[row,col,id]'])[counter])\n",
    "            )\n",
    "            counter += 1     \n",
    "\n",
    "\n",
    "    return nwbfile\n",
    "\n",
    "df = pd.read_excel( os.path.dirname(cwd)+'/pico_inventory.xlsx' , sheet_name='Sheet2')\n",
    "df['prom'] = ''\n",
    "df['prom train'] = ''\n",
    "df['prom test'] = ''\n",
    "df['removed nan reps/session'] = ''\n",
    "df['number of qc channels'] = ''\n",
    "df['number of total reps'] = ''\n",
    "df['sessions kept'] = ''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create per experiment nwb file with good channels only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_processed(df, exp_path):\n",
    "    imageset = os.path.basename(exp_path).split('.')[0].split('_')[1:]\n",
    "    \n",
    "    if len(imageset) == 1: imageset = imageset[0]\n",
    "    elif len(imageset) > 1: imageset = '_'.join(imageset)\n",
    "    mask = df['ImageSet'] == imageset\n",
    "    index = df.index[mask].tolist()[0]\n",
    "    if df.iloc[index]['proc_nwb'].startswith(\"P-Values added.\"): return True\n",
    "    else: \n",
    "        print(\"    \", df.iloc[index]['proc_nwb'])\n",
    "        return False\n",
    "\n",
    "def update_sheet(df, exp_path, text, which_nwb):\n",
    "    imageset = os.path.basename(exp_path).split('.')[0].split('_')[1:]\n",
    "    if len(imageset) == 1: imageset = imageset[0]\n",
    "    elif len(imageset) > 1: imageset = '_'.join(imageset)\n",
    "    mask = df['ImageSet'] == imageset\n",
    "    index = df.index[mask].tolist()[0]\n",
    "    df.at[index, which_nwb] = text\n",
    "\n",
    "def find_norm_with_date(realdate):\n",
    "    all_paths = []\n",
    "    normalizer_file_paths = glob.glob(os.path.join(root_dir, '[norm]*', '*', '*', '[!h5]*'))\n",
    "    for norm_file_path in normalizer_file_paths:\n",
    "        date, time = os.path.basename(norm_file_path).split('.')[-2].split('_')\n",
    "        if date == realdate: \n",
    "            all_paths.append(norm_file_path)\n",
    "\n",
    "    if len(all_paths) == 0: return None   \n",
    "    \n",
    "    return all_paths\n",
    "\n",
    "def zscore_psth_using_normalizers(psth, realdate, integration_start_ms, integration_end_ms):\n",
    "\n",
    "    norm_paths = find_norm_with_date(realdate)\n",
    "    for norm_path in norm_paths:\n",
    "\n",
    "        if norm_path != None:\n",
    "            norm_nwb_file_path = glob.glob(os.path.join(norm_path, '*[nwb]'))[0]\n",
    "            \n",
    "            print(f\"Using normalizer file {os.path.basename(norm_nwb_file_path)}\")\n",
    "            io = NWBHDF5IO(norm_nwb_file_path, \"r\") \n",
    "            norm_nwbfile = io.read()\n",
    "            try: \n",
    "                normalizer_psth = norm_nwbfile.scratch['psth'][:]\n",
    "                normalizer_meta = norm_nwbfile.scratch['psth meta'][:]\n",
    "                io.close()\n",
    "            except Exception as error: print(error)\n",
    "            \n",
    "            if normalizer_psth.shape[-1] != psth.shape[-1]: continue \n",
    "            # print(normalizer_psth.shape)\n",
    "           \n",
    "            '''\n",
    "            This part of the code has been recycled from Sachi's Code: \n",
    "            /spike-tools-chong/spike_tools/utils/spikeutils.py/combine_sessions()\n",
    "            '''\n",
    "            \n",
    "            assert len(normalizer_psth.shape) == 4 , 'Normalizer PSTH has wrong shape.'         # num_images x num_repetitions x num_timebins x num_channels\n",
    "            \n",
    "            timebase = np.arange(int(normalizer_meta[0]), int(normalizer_meta[1]), int(normalizer_meta[2]))\n",
    "            t_cols = np.where((timebase >= integration_start_ms) & (timebase < integration_end_ms))[0]\n",
    "\n",
    "            if normalizer_psth.shape[0] == 86: # norm_FOSS: 85 normalizers images + 1 gray image\n",
    "                images_ids = np.arange(0,86)\n",
    "                images_no_grey = np.where(images_ids != 26)[0]\n",
    "                normalizer_p_no_grey = normalizer_psth[images_no_grey,:,:,:] # Select all images except grey (#26)\n",
    "            \n",
    "            elif normalizer_psth.shape[0] == 26: # norm_HVM\n",
    "                normalizer_p_no_grey = normalizer_psth[:-1,:,:,:] # Select all images except grey (#26)\n",
    "            \n",
    "            #print(normalizer_p_no_grey[:, :, t_cols, :])\n",
    "            n_p = np.nanmean(normalizer_p_no_grey[:, :, t_cols, :], 2) # then mean 70-170 time bins\n",
    "            n_p = n_p.reshape(-1, normalizer_p_no_grey.shape[-1])  # Reshape so that first two axes collapse into one\n",
    "\n",
    "            mean_response_normalizer = np.nanmean(n_p, 0)   # Mean across images x reps\n",
    "            std_response_normalizer  = np.nanstd(n_p, 0)    # Std across images x reps\n",
    "\n",
    "            psth = np.subtract(psth, mean_response_normalizer[np.newaxis, np.newaxis, np.newaxis, :])\n",
    "            psth = np.divide(psth, std_response_normalizer[np.newaxis, np.newaxis, np.newaxis, :],\n",
    "                            where=std_response_normalizer!=0)\n",
    "            \n",
    "            return psth, normalizer_psth, normalizer_meta, norm_nwb_file_path\n",
    "\n",
    "        else: \n",
    "            print(f'No normalizer found for day {realdate}.')\n",
    "            return [None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "exp_muri1320.sub_pico has 11 days and 12 sessions\n",
      "     Across: using normalizer file norm_FOSS.sub_pico.20220615_113442.proc.nwb and norm_FOSS.sub_pico.20220706_141433.proc.nwb \n",
      "Number of channels do not match. 288 != 192 \n",
      "Across: using normalizer file norm_FOSS.sub_pico.20220615_113442.proc.nwb and norm_FOSS.sub_pico.20220706_142235.proc.nwb \n",
      "'psth'\n",
      "No psth available for normalizers ('20220615', '20220706').\n",
      "\n",
      "________________________________________________________________________________\n",
      "exp_ko_context_size.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v40.sub_pico has 1 days and 3 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230928_101016.proc.nwb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155137/3704893679.py:30: RuntimeWarning: Mean of empty slice\n",
      "  n_p = np.nanmean(normalizer_p_no_grey[:, :, t_cols, :], 2) # then mean 70-170 time bins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized and Original PSTH (330, 50, 30, 192) (330, 50, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230928_101016.proc.nwb\n",
      "Normalized and Original PSTH (330, 4, 30, 192) (330, 4, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230928_101016.proc.nwb\n",
      "Normalized and Original PSTH (330, 11, 30, 192) (330, 11, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_muri1320-2023-v1.sub_pico has 4 days and 8 sessions\n",
      "Loading files from day 1/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230127_160227.proc.nwb\n",
      "Normalized and Original PSTH (1320, 4, 30, 192) (1320, 4, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230127_160227.proc.nwb\n",
      "Normalized and Original PSTH (1320, 5, 30, 192) (1320, 5, 30, 192)\n",
      "Loading files from day 2/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230130_140402.proc.nwb\n",
      "Normalized and Original PSTH (1320, 1, 30, 192) (1320, 1, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230130_140402.proc.nwb\n",
      "Normalized and Original PSTH (1320, 2, 30, 192) (1320, 2, 30, 192)\n",
      "Loading files from day 3/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230131_150309.proc.nwb\n",
      "Normalized and Original PSTH (1320, 9, 30, 192) (1320, 9, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230131_150309.proc.nwb\n",
      "Normalized and Original PSTH (1320, 1, 30, 192) (1320, 1, 30, 192)\n",
      "Loading files from day 4/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230201_133533.proc.nwb\n",
      "Normalized and Original PSTH (1320, 5, 30, 192) (1320, 5, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230201_133533.proc.nwb\n",
      "Normalized and Original PSTH (1320, 2, 30, 192) (1320, 2, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_HVM-var6-subset-2023.sub_pico has 1 days and 1 sessions\n",
      "     Experiment is not going on BrainScore\n",
      "________________________________________________________________________________\n",
      "exp_images_in_context.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_muri1320-2023-v0.sub_pico has 3 days and 5 sessions\n",
      "Loading files from day 1/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230125_144402.proc.nwb\n",
      "Normalized and Original PSTH (1320, 4, 30, 192) (1320, 4, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230125_144402.proc.nwb\n",
      "Normalized and Original PSTH (1320, 2, 30, 192) (1320, 2, 30, 192)\n",
      "Loading files from day 2/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230126_150653.proc.nwb\n",
      "Normalized and Original PSTH (1320, 8, 30, 192) (1320, 8, 30, 192)\n",
      "Loading files from day 3/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230124_120714.proc.nwb\n",
      "Normalized and Original PSTH (1320, 1, 30, 192) (1320, 1, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230124_120714.proc.nwb\n",
      "Normalized and Original PSTH (1320, 6, 30, 192) (1320, 6, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_domain-transfer-2023.sub_pico has 12 days and 12 sessions\n",
      "Loading files from day 1/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230216_145353.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 2/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230217_140904.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 3/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230221_124922.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 4/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230222_143642.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 5/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230223_132511.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 6/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230224_142014.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 7/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230227_133424.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 8/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230214_134820.proc.nwb\n",
      "Normalized and Original PSTH (3138, 1, 30, 192) (3138, 1, 30, 192)\n",
      "Loading files from day 9/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230215_145155.proc.nwb\n",
      "Normalized and Original PSTH (3138, 1, 30, 192) (3138, 1, 30, 192)\n",
      "Loading files from day 10/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230301_140442.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Loading files from day 11/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230303_152601.proc.nwb\n",
      "Normalized and Original PSTH (3138, 1, 30, 192) (3138, 1, 30, 192)\n",
      "Loading files from day 12/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230302_133030.proc.nwb\n",
      "Normalized and Original PSTH (3138, 3, 30, 192) (3138, 3, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_HVM-var6-2023.sub_pico has 12 days and 12 sessions\n",
      "Loading files from day 1/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230216_145353.proc.nwb\n",
      "Normalized and Original PSTH (2560, 2, 30, 192) (2560, 2, 30, 192)\n",
      "Loading files from day 2/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230217_140904.proc.nwb\n",
      "Normalized and Original PSTH (2560, 3, 30, 192) (2560, 3, 30, 192)\n",
      "Loading files from day 3/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230221_124922.proc.nwb\n",
      "Normalized and Original PSTH (2560, 2, 30, 192) (2560, 2, 30, 192)\n",
      "Loading files from day 4/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230222_143642.proc.nwb\n",
      "Normalized and Original PSTH (2560, 2, 30, 192) (2560, 2, 30, 192)\n",
      "Loading files from day 5/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230223_132511.proc.nwb\n",
      "Normalized and Original PSTH (2560, 3, 30, 192) (2560, 3, 30, 192)\n",
      "Loading files from day 6/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230224_142014.proc.nwb\n",
      "Normalized and Original PSTH (2560, 3, 30, 192) (2560, 3, 30, 192)\n",
      "Loading files from day 7/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230214_134820.proc.nwb\n",
      "Normalized and Original PSTH (2560, 1, 30, 192) (2560, 1, 30, 192)\n",
      "Loading files from day 8/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230215_145155.proc.nwb\n",
      "Normalized and Original PSTH (2560, 3, 30, 192) (2560, 3, 30, 192)\n",
      "Loading files from day 9/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230227_133424.proc.nwb\n",
      "Normalized and Original PSTH (2560, 3, 30, 192) (2560, 3, 30, 192)\n",
      "Loading files from day 10/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230301_140442.proc.nwb\n",
      "Normalized and Original PSTH (2560, 3, 30, 192) (2560, 3, 30, 192)\n",
      "Loading files from day 11/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230303_152601.proc.nwb\n",
      "Normalized and Original PSTH (2560, 2, 30, 192) (2560, 2, 30, 192)\n",
      "Loading files from day 12/12\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230302_133030.proc.nwb\n",
      "Normalized and Original PSTH (2560, 3, 30, 192) (2560, 3, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_facesMSFDE.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_facescrub-small.sub_pico has 3 days and 3 sessions\n",
      "Loading files from day 1/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230329_125027.proc.nwb\n",
      "Normalized and Original PSTH (1248, 17, 30, 192) (1248, 17, 30, 192)\n",
      "Loading files from day 2/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230330_122809.proc.nwb\n",
      "Normalized and Original PSTH (1248, 15, 30, 192) (1248, 15, 30, 192)\n",
      "Loading files from day 3/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230331_133552.proc.nwb\n",
      "Normalized and Original PSTH (1248, 18, 30, 192) (1248, 18, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_shinecut.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_images_in_context2.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_objectsize.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_novel500.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_images_in_context3.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_motionset1.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v24.sub_pico has 3 days and 3 sessions\n",
      "Loading files from day 1/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230606_134244.proc.nwb\n",
      "Normalized and Original PSTH (11000, 2, 30, 192) (11000, 2, 30, 192)\n",
      "Loading files from day 2/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230607_103909.proc.nwb\n",
      "Normalized and Original PSTH (11000, 2, 30, 192) (11000, 2, 30, 192)\n",
      "Loading files from day 3/3\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230605_122450.proc.nwb\n",
      "Normalized and Original PSTH (11000, 2, 30, 192) (11000, 2, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_Alireza_paradigm1.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: too many indices for array: array is 3-dimensional, but 4 were indexed\n",
      "________________________________________________________________________________\n",
      "exp_Alireza_paradigm2.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: too many indices for array: array is 3-dimensional, but 4 were indexed\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v26.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v26.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v27.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v30.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_moca.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_mayo.sub_pico has 7 days and 14 sessions\n",
      "     Experiment is not going on BrainScore\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v31.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v31.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v32.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v32.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v33.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_chong_d0_v7.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v7_chong_dryrun.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d2_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d3_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: too many indices for array: array is 3-dimensional, but 4 were indexed\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d4_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d5_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: too many indices for array: array is 3-dimensional, but 4 were indexed\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d6_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d7_v7_chong.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_gratingsAdap_s1.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_gratingsAdap_s2.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_gratingsAdap_s3.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_gratingsAdap_s4.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_gratingsAdap_s5.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_faceemovids.sub_pico has 1 days and 2 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230908_094658.proc.nwb\n",
      "Normalized and Original PSTH (360, 4, 100, 192) (360, 4, 100, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230908_094658.proc.nwb\n",
      "Normalized and Original PSTH (360, 10, 100, 192) (360, 10, 100, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v34.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v34.sub_pico has 1 days and 2 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230906_093230.proc.nwb\n",
      "Normalized and Original PSTH (330, 53, 30, 192) (330, 53, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230906_093230.proc.nwb\n",
      "Normalized and Original PSTH (330, 12, 30, 192) (330, 12, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v35.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v35.sub_pico has 1 days and 2 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230913_104325.proc.nwb\n",
      "Normalized and Original PSTH (330, 40, 30, 192) (330, 40, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230913_104325.proc.nwb\n",
      "Normalized and Original PSTH (330, 25, 30, 192) (330, 25, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v36.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v37.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v37.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v39.sub_pico has 1 days and 2 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230922_134247.proc.nwb\n",
      "Normalized and Original PSTH (550, 10, 30, 192) (550, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230922_134247.proc.nwb\n",
      "Normalized and Original PSTH (550, 30, 30, 192) (550, 30, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_afv.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_Kar_2023_degraded.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d0_v40.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_robustness_guy_d1_v41.sub_pico has 1 days and 2 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230929_095756.proc.nwb\n",
      "Normalized and Original PSTH (550, 16, 30, 192) (550, 16, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230929_095756.proc.nwb\n",
      "Normalized and Original PSTH (550, 24, 30, 192) (550, 24, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_gestalt.sub_pico has 4 days and 4 sessions\n",
      "Loading files from day 1/4\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230501_113950.proc.nwb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_155137/3704893679.py:37: RuntimeWarning: Mean of empty slice\n",
      "  n_p = np.nanmean(normalizer_p_no_grey[:, :, t_cols, :], 2) # then mean 70-170 time bins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized and Original PSTH (300, 8, 200, 192) (300, 8, 200, 192)\n",
      "Loading files from day 2/4\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230502_145301.proc.nwb\n",
      "Normalized and Original PSTH (300, 8, 200, 192) (300, 8, 200, 192)\n",
      "Loading files from day 3/4\n",
      "An error occurred: 'psth'\n",
      "Loading files from day 4/4\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230504_114437.proc.nwb\n",
      "Normalized and Original PSTH (300, 8, 200, 192) (300, 8, 200, 192)\n",
      "________________________________________________________________________________\n",
      "exp_IAPS.sub_pico has 2 days and 2 sessions\n",
      "Loading files from day 1/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230517_115439.proc.nwb\n",
      "Normalized and Original PSTH (1183, 15, 30, 192) (1183, 15, 30, 192)\n",
      "Loading files from day 2/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230518_103908.proc.nwb\n",
      "Normalized and Original PSTH (1183, 15, 30, 192) (1183, 15, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_IAPS-200on.sub_pico has 2 days and 2 sessions\n",
      "Loading files from day 1/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230608_150826.proc.nwb\n",
      "Normalized and Original PSTH (1183, 18, 50, 192) (1183, 18, 50, 192)\n",
      "Loading files from day 2/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (1183, 12, 50, 192) (1183, 12, 50, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_object_relations.sub_pico has 3 days and 3 sessions\n",
      "Loading files from day 1/3\n",
      "An error occurred: 'psth'\n",
      "Loading files from day 2/3\n",
      "An error occurred: 'psth'\n",
      "Loading files from day 3/3\n",
      "An error occurred: 'psth'\n",
      "________________________________________________________________________________\n",
      "exp_shapenet360.sub_pico has 3 days and 3 sessions\n",
      "Loading files from day 1/3\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230524_114326.proc.nwb\n",
      "Normalized and Original PSTH (1561, 10, 50, 192) (1561, 10, 50, 192)\n",
      "Loading files from day 2/3\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230525_110442.proc.nwb\n",
      "Normalized and Original PSTH (1561, 10, 50, 192) (1561, 10, 50, 192)\n",
      "Loading files from day 3/3\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230526_113144.proc.nwb\n",
      "Normalized and Original PSTH (1561, 10, 50, 192) (1561, 10, 50, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_Co3D.sub_pico has 4 days and 4 sessions\n",
      "Loading files from day 1/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230627_112925.proc.nwb\n",
      "Normalized and Original PSTH (319, 7, 200, 192) (319, 7, 200, 192)\n",
      "Loading files from day 2/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230628_124854.proc.nwb\n",
      "Normalized and Original PSTH (319, 7, 200, 192) (319, 7, 200, 192)\n",
      "Loading files from day 3/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230629_132813.proc.nwb\n",
      "Normalized and Original PSTH (319, 7, 200, 192) (319, 7, 200, 192)\n",
      "Loading files from day 4/4\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230630_135832.proc.nwb\n",
      "Normalized and Original PSTH (319, 7, 200, 192) (319, 7, 200, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_shapegen_static.sub_pico has 2 days and 2 sessions\n",
      "Loading files from day 1/2\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230316_120749.proc.nwb\n",
      "Normalized and Original PSTH (200, 30, 30, 192) (200, 30, 30, 192)\n",
      "Loading files from day 2/2\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230320_123005.proc.nwb\n",
      "Normalized and Original PSTH (200, 30, 30, 192) (200, 30, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_shapegen_dynamic.sub_pico has 2 days and 2 sessions\n",
      "Loading files from day 1/2\n",
      "An error occurred: 'psth'\n",
      "Loading files from day 2/2\n",
      "An error occurred: 'psth'\n",
      "________________________________________________________________________________\n",
      "exp_NSD-COCO.sub_pico has 3 days and 3 sessions\n",
      "Loading files from day 1/3\n",
      "An error occurred: 'psth'\n",
      "Loading files from day 2/3\n",
      "An error occurred: 'psth'\n",
      "Loading files from day 3/3\n",
      "An error occurred: 'psth'\n",
      "________________________________________________________________________________\n",
      "exp_1_shapes.sub_pico has 3 days and 3 sessions\n",
      "Loading files from day 1/3\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230323_114236.proc.nwb\n",
      "Normalized and Original PSTH (1021, 10, 50, 192) (1021, 10, 50, 192)\n",
      "Loading files from day 2/3\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230322_114919.proc.nwb\n",
      "Normalized and Original PSTH (1021, 10, 50, 192) (1021, 10, 50, 192)\n",
      "Loading files from day 3/3\n",
      "Within: using normalizer file norm_HVM.sub_pico.20230321_121454.proc.nwb\n",
      "Normalized and Original PSTH (1021, 10, 50, 192) (1021, 10, 50, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_physion.sub_pico has 10 days and 10 sessions\n",
      "     Experiment is not going on BrainScore\n",
      "________________________________________________________________________________\n",
      "exp_emogan.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_food.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_IAPS100.sub_pico has 1 days and 5 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (101, 10, 30, 192) (101, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (101, 10, 30, 192) (101, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (101, 10, 30, 192) (101, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (101, 10, 30, 192) (101, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (101, 10, 30, 192) (101, 10, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_IAPS100c.sub_pico has 1 days and 5 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (103, 10, 30, 192) (103, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (103, 10, 30, 192) (103, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (103, 10, 30, 192) (103, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (103, 10, 30, 192) (103, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230609_133109.proc.nwb\n",
      "Normalized and Original PSTH (103, 10, 30, 192) (103, 10, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_flicker.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: 'RecordingQualityArray'\n",
      "________________________________________________________________________________\n",
      "exp_sine_wave.sub_pico has 2 days and 2 sessions\n",
      "     No normalizer found for day ('20230614', '20230803').\n",
      "\n",
      "________________________________________________________________________________\n",
      "exp_square_sinewave.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence-200on.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: 'RecordingQualityArray'\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence2.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: 'RecordingQualityArray'\n",
      "________________________________________________________________________________\n",
      "exp_oasis900.sub_pico has 2 days and 2 sessions\n",
      "Loading files from day 1/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230508_120630.proc.nwb\n",
      "Normalized and Original PSTH (900, 25, 30, 192) (900, 25, 30, 192)\n",
      "Loading files from day 2/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230509_112900.proc.nwb\n",
      "Normalized and Original PSTH (900, 25, 30, 192) (900, 25, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_oasis100c.sub_pico has 1 days and 5 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_oasis100o.sub_pico has 1 days and 5 sessions\n",
      "Loading files from day 1/1\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230510_125550.proc.nwb\n",
      "Normalized and Original PSTH (100, 10, 30, 192) (100, 10, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_oasis900scrambled.sub_pico has 5 days and 5 sessions\n",
      "Loading files from day 1/5\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230517_115439.proc.nwb\n",
      "Normalized and Original PSTH (1800, 4, 30, 192) (1800, 4, 30, 192)\n",
      "Loading files from day 2/5\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230516_120137.proc.nwb\n",
      "Normalized and Original PSTH (1800, 13, 30, 192) (1800, 13, 30, 192)\n",
      "Loading files from day 3/5\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230802_124516.proc.nwb\n",
      "Normalized and Original PSTH (1800, 10, 30, 192) (1800, 10, 30, 192)\n",
      "Loading files from day 4/5\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230803_105856.proc.nwb\n",
      "Normalized and Original PSTH (1800, 12, 30, 192) (1800, 12, 30, 192)\n",
      "Loading files from day 5/5\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230515_133554.proc.nwb\n",
      "Normalized and Original PSTH (1800, 13, 30, 192) (1800, 13, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_oasis900rotated.sub_pico has 2 days and 2 sessions\n",
      "Loading files from day 1/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230831_121100.proc.nwb\n",
      "Normalized and Original PSTH (1800, 15, 30, 192) (1800, 15, 30, 192)\n",
      "Loading files from day 2/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230901_115230.proc.nwb\n",
      "Normalized and Original PSTH (1800, 15, 30, 192) (1800, 15, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_oasis900_200on.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence2-200on.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: 'RecordingQualityArray'\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence3.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "An error occurred: 'RecordingQualityArray'\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence4.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence6.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence7.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_monkeyvalence8.sub_pico has 1 days and 1 sessions\n",
      "Loading files from day 1/1\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n",
      "________________________________________________________________________________\n",
      "exp_oasis900scrambled_200on.sub_pico has 2 days and 2 sessions\n",
      "Loading files from day 1/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230821_120107.proc.nwb\n",
      "Normalized and Original PSTH (1800, 15, 30, 192) (1800, 15, 30, 192)\n",
      "Loading files from day 2/2\n",
      "Within: using normalizer file norm_FOSS.sub_pico.20230822_153931.proc.nwb\n",
      "Normalized and Original PSTH (1800, 10, 30, 192) (1800, 10, 30, 192)\n",
      "Saving combined NWB Files.\n",
      "Combined file saved.\n",
      "Train file saved.\n",
      "Test file saved.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------ \n",
    "# Load excel file and find all experiment names goind on brainscore. \n",
    "# ------------------------------------------------------------------------------ \n",
    "list_of_bs_exp_names = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['BrainScore']=='Y': list_of_bs_exp_names.append(row['ImageSet'])\n",
    "    \n",
    "experiment_file_paths = glob.glob(os.path.join(root_dir, '[exp]*', '*'))\n",
    "experiment_file_paths = [d for d in experiment_file_paths if 'VideoStimulusSet' not in os.path.basename(d)]\n",
    "\n",
    "for experiment_path in experiment_file_paths: \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Skip files, which do not go on BrainScore.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    experiment_name =  \"_\".join(os.path.basename(experiment_path).split('.')[0].split('_')[1:])\n",
    "\n",
    "    if experiment_name not in list_of_bs_exp_names: \n",
    "        continue \n",
    "        \n",
    "    # if os.path.basename(experiment_path)!='exp_NSD-COCO.sub_pico': continue \n",
    "\n",
    "    days    = glob.glob(os.path.join(experiment_path, '*[!npy][!nwb]'))\n",
    "    n_days  = len(days)\n",
    "    n_sessions = 0\n",
    "    for day in days :\n",
    "        n_sessions += len(glob.glob(os.path.join(experiment_path, day, '*',  '*proc.nwb')))\n",
    "\n",
    "    print('________________________________________________________________________________')\n",
    "    print(f'{os.path.basename(experiment_path)} has {n_days} days and {n_sessions} sessions')\n",
    "    \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Skip files, which have no nwb files or if combined nwb file already exists.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    if n_sessions == 0: continue\n",
    "    combined_exists = False\n",
    "    train_exists    = False\n",
    "    test_exists     = False\n",
    "    min_QC_channel_tresh_local = 0.0\n",
    "\n",
    "    if os.path.isfile(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom.nwb\")):\n",
    "        print(f'Prom nwb file exists for {os.path.basename(experiment_path)}')\n",
    "        io = NWBHDF5IO(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom.nwb\"), \"r\") \n",
    "        combined_nwb = io.read()\n",
    "        min_QC_channel_tresh_local = float(combined_nwb.scratch['PSTHs_QualityApproved_SessionMerged'].description.split(' %')[-2].split(' ')[-1])/100\n",
    "        total_reps = combined_nwb.scratch['PSTHs_QualityApproved_SessionMerged'][:].shape[1]\n",
    "        total_QC_channels = combined_nwb.scratch['PSTHs_QualityApproved_SessionMerged'][:].shape[3]\n",
    "        pattern = r'\\d{8}_\\d{6}'\n",
    "        dates_times = re.findall(pattern, combined_nwb.scratch['PSTHs_QualityApproved_SessionMerged'].description)\n",
    "\n",
    "        io.close()\n",
    "\n",
    "        print(f'QC channel threshold: {min_QC_channel_tresh_local*100} %')\n",
    "        print(f'Total reps number, QC channel number: {total_reps}, {total_QC_channels}')\n",
    "        update_sheet(df, experiment_path, 'Done.', 'prom')\n",
    "        update_sheet(df, experiment_path, total_QC_channels , 'number of qc channels')\n",
    "        update_sheet(df, experiment_path, str(dates_times) , 'sessions kept')\n",
    "        update_sheet(df, experiment_path, total_reps , 'number of total reps')\n",
    "        combined_exists = True\n",
    "\n",
    "    if os.path.isfile(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom_train.nwb\")):\n",
    "        print(f'Prom train nwb file exists for {os.path.basename(experiment_path)}')\n",
    "        # io = NWBHDF5IO(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom_train.nwb\"), \"r\") \n",
    "        # combined_nwb_train = io.read()\n",
    "        update_sheet(df, experiment_path, 'Done', 'prom train')\n",
    "        train_exists    = True\n",
    "\n",
    "    if os.path.isfile(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom_test.nwb\")):\n",
    "        print(f'Prom test nwb file exists for {os.path.basename(experiment_path)}')\n",
    "        io = NWBHDF5IO(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom_test.nwb\"), \"r\") \n",
    "        combined_nwb_test = io.read()\n",
    "        update_sheet(df, experiment_path, 'Done.', 'prom test')\n",
    "        test_exists     = True\n",
    "        \n",
    "\n",
    "    if combined_exists and train_exists and test_exists: continue \n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Create combined nwb files, if all previous steps are taken.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    if experiment_processed(df, experiment_path) == True: \n",
    "        first_day = days[0].split('.')[-1]\n",
    "        with open(os.path.join(glob.glob(os.path.join(experiment_path, days[0], '*'))[0],f\"config_nwb.yaml\") , \"r\") as f:\n",
    "                first_rec_config = yaml.load(f, Loader = yaml.FullLoader)\n",
    "                if combined_exists == False: combined_nwb        = create_prom_nwb(first_rec_config, glob.glob(os.path.join(experiment_path, days[0], '*'))[0], os.path.basename(days[0]).split('.')[0])\n",
    "                if train_exists    == False: combined_nwb_train  = create_prom_nwb(first_rec_config, glob.glob(os.path.join(experiment_path, days[0], '*'))[0], os.path.basename(days[0]).split('.')[0])\n",
    "                if test_exists     == False: combined_nwb_test   = create_prom_nwb(first_rec_config, glob.glob(os.path.join(experiment_path, days[0], '*'))[0], os.path.basename(days[0]).split('.')[0])\n",
    "            \n",
    "        combined_good_channel_ids       = []\n",
    "        combined_date_times             = []\n",
    "        combined_normalizer_date_times  = []\n",
    "        train_IDs                       = []\n",
    "        test_IDs                        = []\n",
    "        removed_nan_reps                = []\n",
    "        \n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Load all the proc nwb files and combine them to a prom nwb file.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "\n",
    "        pass_saving       = False\n",
    "        all_good_channels = []\n",
    "        all_psth          = []\n",
    "        all_psth_zscored  = []\n",
    "        all_psth_normalizers = []\n",
    "\n",
    "        for day, i_day in zip(days, range(n_days)) :\n",
    "            print(f'Loading files from day {i_day+1}/{n_days}')\n",
    "            exp_nwb_paths = (glob.glob(os.path.join(experiment_path, day, '*',  '*proc.nwb')))\n",
    "            \n",
    "            for exp_nwb_path in exp_nwb_paths:\n",
    "                \n",
    "                # ------------------------------------------------------------------------------ \n",
    "                # Load recording nwb file.\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                try:\n",
    "                    io = NWBHDF5IO(exp_nwb_path, \"r\") \n",
    "                    exp_nwbfile = io.read()\n",
    "                except:\n",
    "                    print(f\"Cannot open nwb. file {os.path.basename(exp_nwb_path)}\")\n",
    "                    io.close()\n",
    "                    update_sheet(df, experiment_path, f\"Cannot open nwb. file {os.path.basename(exp_nwb_path)}\", 'prom')\n",
    "                    pass_saving = True\n",
    "                    continue \n",
    "                    \n",
    "                try: \n",
    "                    psth        = exp_nwbfile.scratch['psth'][:]\n",
    "                    psth_meta   = exp_nwbfile.scratch['psth meta'][:]\n",
    "\n",
    "                    if os.path.basename(experiment_path) == 'exp_muri1320.sub_pico' and i_day == 0:\n",
    "                        # This particular session for this experiment has been calculated with different \n",
    "                        # parameter settings within an experiment. Therefore, I am manually correcting this previous error. \n",
    "                        psth = psth[:,:,10:40,:]\n",
    "                        psth_meta = np.array([0, 300, 10])\n",
    "                    \n",
    "                    print(f'    PSTH Meta of file {i_day+1}/{n_days}: {psth_meta}')\n",
    "                    \n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    # Define Train and Test set stimulus ids for this stimulus set. This is \n",
    "                    # consistend over the whole project.\n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    if i_day == 0:\n",
    "                        n_stimuli   = psth.shape[0]\n",
    "                        stimulus_id = np.arange(n_stimuli)\n",
    "                        np.random.shuffle(stimulus_id)\n",
    "                        split_index = int(n_stimuli * test_train_split)\n",
    "                        \n",
    "                        train_ids = stimulus_id[:split_index]\n",
    "                        test_ids  = stimulus_id[split_index:]\n",
    "\n",
    "                        train_ids = sorted(train_ids)\n",
    "                        test_ids  = sorted(test_ids)\n",
    "\n",
    "                        train_IDs.append(train_ids)\n",
    "                        test_IDs.append(test_ids)  \n",
    "                    else:\n",
    "                        train_ids = train_IDs[0]\n",
    "                        test_ids  = test_IDs[0]\n",
    "\n",
    "                    # print('    Train IDs: ',train_ids)\n",
    "                    # print('    Test IDs: ', test_ids)\n",
    "\n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    # Calculate z-scored psth.\n",
    "                    # ------------------------------------------------------------------------------                     \n",
    "                    current_day = day.split('.')[-1]\n",
    "                    nan_exists = True\n",
    "                    nan_rep_counter = 0\n",
    "                    while nan_exists == True:\n",
    "                        if np.isnan(psth).sum() != 0: \n",
    "                            nan_rep_counter += 1\n",
    "                            psth = psth[:,:-1,:,:] # remove last rep of nans \n",
    "                        else: nan_exists = False\n",
    "                    removed_nan_reps.append(nan_rep_counter)\n",
    "                    print(f'    Removed nan-reps: {nan_rep_counter}')\n",
    "                    psth_zscored, normalizer_psth, normalizer_meta, normalizer_filename = zscore_psth_using_normalizers(psth, current_day, integration_start_ms, integration_end_ms)\n",
    "                    \n",
    "                    # TO DO!! CHECK IF THIS IS NECESSARY\n",
    "                    if normalizer_meta[0]!=0 and normalizer_meta[1]!=300:\n",
    "                        normalizer_psth = normalizer_psth[:,:,int(-normalizer_meta[0]/10):int(-normalizer_meta[0]/10)+30,:]\n",
    "                        normalizer_meta = np.array([0, 300, 10])\n",
    "                    \n",
    "                    #all_meta_normalizers.append(normalizer_meta)\n",
    "                    all_psth_normalizers.append(normalizer_psth)\n",
    "                    combined_normalizer_date_times.append(os.path.basename(normalizer_filename).split('.')[-3])\n",
    "\n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    # Remove all the bad channels for the z-scored and not z-scored psth. \n",
    "                    # This is the quality checked (QC) psth\n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    p_values = exp_nwbfile.scratch['PValuesPerChannel'][:]\n",
    "                    good_channel_ids = p_values<p_value_threshold\n",
    "                    combined_good_channel_ids.append(good_channel_ids)\n",
    "                    combined_date_times.append(os.path.basename(exp_nwb_path).split('.')[-3])\n",
    "                    \n",
    "                    psth_QC         = psth[:,:,:,good_channel_ids] \n",
    "                    psth_zscored_QC = psth_zscored[:,:,:,good_channel_ids] \n",
    "\n",
    "                    print(f\"    psth_zscored and original psth shape:   {psth_zscored.shape} {psth.shape}\")\n",
    "                    print(f'    psth_QC and original psth shape:        {psth_QC.shape} {psth.shape}')\n",
    "                    print(f'    psth_zscored_QC and psth_zscored shape: {psth_zscored_QC.shape} {psth_zscored.shape}')\n",
    "                    print(f'    normalizer psth shape and meta:         {normalizer_psth.shape} {normalizer_meta}')\n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    # Remove the last 'nan' rep.\n",
    "                    # ------------------------------------------------------------------------------\n",
    "                    for i in range(nan_rep_counter):\n",
    "                        if np.isnan(psth_QC).sum() != 0: psth_QC = psth_QC[:,:-1,:,:] # remove last rep of nans \n",
    "                        if np.isnan(psth_zscored_QC).sum() != 0: psth_zscored_QC = psth_zscored_QC[:,:-1,:,:] # remove last rep of nans \n",
    "                        if np.isnan(psth_zscored).sum() != 0: psth_zscored = psth_zscored_QC[:,:-1,:,:] # remove last rep of nans \n",
    "                        \n",
    "                    assert np.isnan(psth_QC).sum() == 0, \"Nan Repetitions are still present. This should not be the case.\"\n",
    "\n",
    "                    print(f'    psth_QC shape after removing nan-reps: {psth_QC.shape}')\n",
    "                    all_psth.append(psth)\n",
    "                    all_psth_zscored.append(psth_zscored)                  \n",
    "\n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    # Add to psth_QC and psth_zscored_QC combined nwb files. \n",
    "                    # One file containing all the data, one test and one train.\n",
    "                    # ------------------------------------------------------------------------------ \n",
    "                    def description_QC(psth_meta, z_scored, train_test): \n",
    "                        if z_scored == True: psth_array_type = 'Z-scored (using the normalizer recordings from this day) '\n",
    "                        else: psth_array_type =''\n",
    "                        if train_test != 'combined': \n",
    "                            StimIDs = 'A corresponding index-to-stimulus ID mapping is available in the array StimuliIDs. '\n",
    "                        else: StimIDs =''\n",
    "                        return f\"{psth_array_type}PSTH array with dimensions corresponding to \\\n",
    "                            [stimuli x repetitions x time bins x good quality channels], \\\n",
    "                            where 'quality channels' are those with p-values (calculated using the normalizer recordings) less \\\n",
    "                            than {p_value_threshold}, indicating high signal quality.\\\n",
    "                            {StimIDs}The PSTH meta are the following for [start_time_ms, stop_time_ms, tb_ms]: {psth_meta}\"\n",
    "                        \n",
    "                    if combined_exists == False:\n",
    "                        combined_nwb.add_scratch(\n",
    "                            psth_QC,\n",
    "                            name=f\"PSTHs_QualityApproved_{os.path.basename(exp_nwb_path).split('.')[-3]}\",\n",
    "                            description=description_QC(psth_meta, z_scored=False, train_test='combined'))\n",
    "                        combined_nwb.add_scratch(\n",
    "                            psth_zscored_QC,\n",
    "                            name=f\"PSTHs_QualityApproved_ZScored_{os.path.basename(exp_nwb_path).split('.')[-3]}\",\n",
    "                            description=description_QC(psth_meta, z_scored=True, train_test='combined'))\n",
    "                    if train_exists == False:\n",
    "                        combined_nwb_train.add_scratch(\n",
    "                            psth_QC[train_ids,:,:,:],\n",
    "                            name=f\"PSTHs_QualityApproved_{os.path.basename(exp_nwb_path).split('.')[-3]}\",\n",
    "                            description=description_QC(psth_meta, z_scored=False, train_test='train'))\n",
    "                        combined_nwb_train.add_scratch(\n",
    "                            psth_zscored_QC[train_ids,:,:,:],\n",
    "                            name=f\"PSTHs_QualityApproved_ZScored_{os.path.basename(exp_nwb_path).split('.')[-3]}\",\n",
    "                            description=description_QC(psth_meta, z_scored=True, train_test='train'))\n",
    "                    if test_exists == False:\n",
    "                        combined_nwb_test.add_scratch(\n",
    "                            psth_QC[test_ids,:,:,:],\n",
    "                            name=f\"PSTHs_QualityApproved_{os.path.basename(exp_nwb_path).split('.')[-3]}\",\n",
    "                            description=description_QC(psth_meta, z_scored=False, train_test='test'))\n",
    "                        combined_nwb_test.add_scratch(\n",
    "                            psth_zscored_QC[test_ids,:,:,:],\n",
    "                            name=f\"PSTHs_QualityApproved_ZScored_{os.path.basename(exp_nwb_path).split('.')[-3]}\",\n",
    "                            description=description_QC(psth_meta, z_scored=True, train_test='test'))\n",
    "                    \n",
    "                except Exception as error:\n",
    "                    print(\"An error occurred:\", error) \n",
    "                    io.close()\n",
    "                    pass_saving = True\n",
    "                    update_sheet(df, experiment_path, error, 'prom')\n",
    "                    continue \n",
    "\n",
    "                io.close()\n",
    "    \n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Combine all sessions using the common good channels. This step is also performed \n",
    "        # if only one session is available, to keep the standardized format of the prom.nwb files.\n",
    "        # If there is a day, which has very low number of good channels, we will discard that \n",
    "        # session in the prom psth. \n",
    "        # ------------------------------------------------------------------------------\n",
    "        if pass_saving == True: continue\n",
    "        \n",
    "        prom_psth_QC          = []\n",
    "        prom_psth_QC_train    = []\n",
    "        prom_psth_QC_test     = []\n",
    "            \n",
    "        prom_psth_zscored_QC          = []\n",
    "        prom_psth_zscored_QC_train    = []\n",
    "        prom_psth_zscored_QC_test     = []\n",
    "\n",
    "        prom_psth_normalizers         = []\n",
    "\n",
    "        if min_QC_channel_tresh_local == 0.0: min_QC_channel_tresh_local = min_QC_channel_tresh\n",
    "\n",
    "        # find the session id's to keep, i.e. sessions which have enough good channels\n",
    "        n_channels    = len(combined_good_channel_ids[0])\n",
    "        min_nchannels = int(n_channels*min_QC_channel_tresh_local)\n",
    "        proc_session_mask_keep  = np.full(len(combined_good_channel_ids), False, dtype=bool) \n",
    "        for i_session in range(len(combined_good_channel_ids)):\n",
    "            if sum(combined_good_channel_ids[i_session]) >= min_nchannels: proc_session_mask_keep[i_session] = True\n",
    "\n",
    "           \n",
    "        combined_good_channel_ids_keep = [array for array, include in zip(combined_good_channel_ids, proc_session_mask_keep) if include]\n",
    "        common_QC_channels             = np.logical_and.reduce(combined_good_channel_ids_keep)\n",
    "\n",
    "        # Reduce min_QC_channel_tresh if the number of common quality approved channels is lower than min_number_QC_channel_tresh\n",
    "        if common_QC_channels.sum() < min_number_QC_channel_tresh:\n",
    "            print(f'    Reducing the threshold to {min_QC_channel_tresh_local-0.05}, because number of common QC channels is {common_QC_channels.sum()}')\n",
    "            min_QC_channel_tresh_local = min_QC_channel_tresh_local-0.05\n",
    "            n_channels    = len(combined_good_channel_ids[0])\n",
    "            min_nchannels = int(n_channels*min_QC_channel_tresh_local)\n",
    "            proc_session_mask_keep  = np.full(len(combined_good_channel_ids), False, dtype=bool) \n",
    "            for i_session in range(len(combined_good_channel_ids)):\n",
    "                if sum(combined_good_channel_ids[i_session]) >= min_nchannels: proc_session_mask_keep[i_session] = True\n",
    "                \n",
    "            combined_good_channel_ids_keep = [array for array, include in zip(combined_good_channel_ids, proc_session_mask_keep) if include]\n",
    "            common_QC_channels             = np.logical_and.reduce(combined_good_channel_ids_keep)\n",
    "        \n",
    "        if common_QC_channels.sum() < min_number_QC_channel_tresh: pass_saving == True\n",
    "        \n",
    "        print(f'    Per Session QC channel threshold: {min_QC_channel_tresh_local*100} %')\n",
    "        print('    Number of common QC channels: ', common_QC_channels.sum())\n",
    "\n",
    "        if pass_saving == True: continue\n",
    "            \n",
    "        first_session = True\n",
    "        if n_sessions > 0:\n",
    "            for psth_QC, psth_zscored_QC, psth_normalizer, i in zip(all_psth, all_psth_zscored, all_psth_normalizers, range(len(all_psth))):\n",
    "                if proc_session_mask_keep[i] == True:\n",
    "                    if first_session == True:\n",
    "                        prom_psth_QC       = psth_QC[:,:,:,common_QC_channels]\n",
    "                        prom_psth_QC_train = prom_psth_QC[list(train_ids),:,:,:]\n",
    "                        prom_psth_QC_test  = prom_psth_QC[list(test_ids),:,:,:]\n",
    "\n",
    "                        prom_psth_zscored_QC       = psth_zscored_QC[:,:,:,common_QC_channels]\n",
    "                        prom_psth_zscored_QC_train = prom_psth_zscored_QC[list(train_ids),:,:,:]\n",
    "                        prom_psth_zscored_QC_test  = prom_psth_zscored_QC[list(test_ids),:,:,:]\n",
    "\n",
    "                        prom_psth_normalizers = psth_normalizer\n",
    "\n",
    "                        first_session = False\n",
    "                    else:\n",
    "                        prom_psth_QC_temp          = psth_QC[:,:,:,common_QC_channels]\n",
    "                        prom_psth_zscored_QC_temp  = psth_zscored_QC[:,:,:,common_QC_channels]\n",
    "\n",
    "                        prom_psth_QC       = np.hstack((prom_psth_QC,       prom_psth_QC_temp))\n",
    "                        prom_psth_QC_train = np.hstack((prom_psth_QC_train, prom_psth_QC_temp[list(train_ids),:,:,:]))\n",
    "                        prom_psth_QC_test  = np.hstack((prom_psth_QC_test,  prom_psth_QC_temp[list(test_ids),:,:,:]))\n",
    "            \n",
    "                        prom_psth_zscored_QC       = np.hstack((prom_psth_zscored_QC,       prom_psth_zscored_QC_temp))\n",
    "                        prom_psth_zscored_QC_train = np.hstack((prom_psth_zscored_QC_train, prom_psth_zscored_QC_temp[list(train_ids),:,:,:]))\n",
    "                        prom_psth_zscored_QC_test  = np.hstack((prom_psth_zscored_QC_test,  prom_psth_zscored_QC_temp[list(test_ids),:,:,:]))\n",
    "\n",
    "                        prom_psth_normalizers = np.hstack((prom_psth_normalizers,psth_normalizer))\n",
    "        if n_sessions == 1:\n",
    "            proc_session_mask_keep[0] = True\n",
    "            common_QC_channels = combined_good_channel_ids[0]\n",
    "            psth_QC         = all_psth[0]\n",
    "            psth_zscored_QC = all_psth_zscored[0]\n",
    "            psth_normalizer = all_psth_normalizers[0]\n",
    "            \n",
    "            prom_psth_QC       = psth_QC[:,:,:,common_QC_channels]\n",
    "            prom_psth_QC_train = prom_psth_QC[list(train_ids),:,:,:]\n",
    "            prom_psth_QC_test  = prom_psth_QC[list(test_ids),:,:,:]\n",
    "\n",
    "            prom_psth_zscored_QC       = psth_zscored_QC[:,:,:,common_QC_channels]\n",
    "            prom_psth_zscored_QC_train = prom_psth_zscored_QC[list(train_ids),:,:,:]\n",
    "            prom_psth_zscored_QC_test  = prom_psth_zscored_QC[list(test_ids),:,:,:]\n",
    "\n",
    "            prom_psth_normalizers = psth_normalizer\n",
    "    \n",
    "        print(\"---------- PROM FILES: -----------------------------------------\")\n",
    "        print(f'    prom_QC shape: {prom_psth_QC.shape}, prom_QC_train shape: {prom_psth_QC_train.shape}, prom_QC_test shape: {prom_psth_QC_test.shape}')\n",
    "        print(f'    prom_zscored_QC shape: {prom_psth_zscored_QC.shape}, prom_zscored_QC_train shape: {prom_psth_zscored_QC_train.shape}, prom_zscored_QC_test shape: {prom_psth_zscored_QC_test.shape}')\n",
    "        print(f'    normalizers shape: {prom_psth_normalizers.shape}')\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Add masks and stimIDs to to combined nwb files.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        def description_prom_QC(psth_meta, combined_date_times, z_scored): \n",
    "            if z_scored == True: psth_array_type = 'Z-scored '\n",
    "            else: psth_array_type =''\n",
    "            return f\"Array of shape [stimuli x all repetitions x time bins x logical 'and' of quality approved channels], \\\n",
    "                containing all {psth_array_type}PSTHs, which have at least {min_QC_channel_tresh_local*100} % quality approved channels, stacked in one matrix.\\\n",
    "                All repetitions are stacked, and the quality approved channels \\\n",
    "                of the recordings are combined using a logical 'and' operation, i.e., \\\n",
    "                common_quality_approved_channels = np.logical_and.reduce(QualityApprovedChannelMasks[days with at least {min_QC_channel_tresh_local*100} % quality approved channels]). \\\n",
    "                The resulting common_quality_approved_channels is used to mask the channel dimensions. \\\n",
    "                Notably, if n_sessions equals 1, the merged PSTH will include the PSTHs_QualityApproved with all quality approved channels,\\\n",
    "                regardless of whether this number meets the {min_QC_channel_tresh_local*100} % threshold.\\\n",
    "                This array contains the quality approved recordings from the following Dates & Times: \\\n",
    "                {[array for array, include in zip(combined_date_times, proc_session_mask_keep) if include]}\\\n",
    "                The PSTH meta are the following for [start_time_ms, stop_time_ms, tb_ms]: {psth_meta}\"\n",
    "        \n",
    "        descripion_QualityApprovedChannelMasks=f\"List of boolean arrays, indexed by recording date & time,\\\n",
    "            each of num channel length, marking 'quality approved channels' as True for p-value < {p_value_threshold}.\\\n",
    "            Dates & times covered:{combined_date_times}\"\n",
    "        description_StimulusID=f\"List of array containting the stimulus IDs that directly correspond to the stimulus \\\n",
    "            indexes in PSTHs_QualityApproved and PSTHs_QualityApproved_ZScored. Each entry in the array aligns with the respective stimulus ID in the PSTH;\\\n",
    "            for example, the first entry in the array corresponds to the stimulus ID of the first entry in the PSTH, and so on.\\\n",
    "            The corresponding stimulus for each stimulus ID is located in the respective stimulus set.\"\n",
    "        description_normalizer=f\"Array of shape [stimuli x all repetitions x time bins x channels], \\\n",
    "            containing PSTH data from the normalizer files, used in the quality check and z-scoring.\\\n",
    "            Normalizer Dates & Times: {combined_normalizer_date_times}\"\n",
    "\n",
    "        \n",
    "        if combined_exists==False: \n",
    "            combined_nwb.add_scratch(\n",
    "                combined_good_channel_ids,\n",
    "                name=f\"QualityApprovedChannelMasks\",\n",
    "                description=descripion_QualityApprovedChannelMasks)\n",
    "                \n",
    "            combined_nwb.add_scratch(\n",
    "                prom_psth_QC,\n",
    "                name=f\"PSTHs_QualityApproved_SessionMerged\",\n",
    "                description=description_prom_QC(psth_meta, combined_date_times, z_scored=False))\n",
    "                    \n",
    "            combined_nwb.add_scratch(\n",
    "                prom_psth_zscored_QC,\n",
    "                name=f\"PSTHs_QualityApproved_ZScored_SessionMerged\",\n",
    "                description=description_prom_QC(psth_meta, combined_date_times, z_scored=True))\n",
    "                        \n",
    "            combined_nwb.add_scratch(\n",
    "                prom_psth_normalizers,\n",
    "                name=f\"PSTHs_Normalizers_SessionMerged\",\n",
    "                description=description_normalizer)\n",
    "\n",
    "        if train_exists == False: \n",
    "            combined_nwb_train.add_scratch(\n",
    "                combined_good_channel_ids,\n",
    "                name=f\"QualityApprovedChannelMasks\",\n",
    "                description=descripion_QualityApprovedChannelMasks)\n",
    "        \n",
    "            combined_nwb_train.add_scratch(\n",
    "                train_ids,\n",
    "                name=f\"StimuliIDs\",\n",
    "                description=description_StimulusID)\n",
    "                \n",
    "            combined_nwb_train.add_scratch(\n",
    "                prom_psth_QC_train,\n",
    "                name=f\"PSTHs_QualityApproved_SessionMerged\",\n",
    "                description=description_prom_QC(psth_meta, combined_date_times, z_scored=False))\n",
    "                    \n",
    "            combined_nwb_train.add_scratch(\n",
    "                prom_psth_zscored_QC_train,\n",
    "                name=f\"PSTHs_QualityApproved_ZScored_SessionMerged\",\n",
    "                description=description_prom_QC(psth_meta, combined_date_times, z_scored=True))\n",
    "\n",
    "            combined_nwb_train.add_scratch(\n",
    "                prom_psth_normalizers,\n",
    "                name=f\"PSTHs_Normalizers_SessionMerged\",\n",
    "                description=description_normalizer)\n",
    "\n",
    "        if test_exists==False:\n",
    "            combined_nwb_test.add_scratch(\n",
    "                combined_good_channel_ids,\n",
    "                name=f\"QualityApprovedChannelMasks\",\n",
    "                description=descripion_QualityApprovedChannelMasks)\n",
    "        \n",
    "            combined_nwb_test.add_scratch(\n",
    "                test_ids,\n",
    "                name=f\"StimuliIDs\",\n",
    "                description=description_StimulusID)\n",
    "        \n",
    "            combined_nwb_test.add_scratch(\n",
    "                prom_psth_QC_test,\n",
    "                name=f\"PSTHs_QualityApproved_SessionMerged\",\n",
    "                description=description_prom_QC(psth_meta, combined_date_times, z_scored=False))\n",
    "\n",
    "            combined_nwb_test.add_scratch(\n",
    "                prom_psth_zscored_QC_test,\n",
    "                name=f\"PSTHs_QualityApproved_ZScored_SessionMerged\",\n",
    "                description=description_prom_QC(psth_meta, combined_date_times, z_scored=True))\n",
    "\n",
    "            combined_nwb_test.add_scratch(\n",
    "                prom_psth_normalizers,\n",
    "                name=f\"PSTHs_Normalizers_SessionMerged\",\n",
    "                description=description_normalizer)        \n",
    "        \n",
    "        update_sheet(df, experiment_path, 'Done.', 'prom')\n",
    "        update_sheet(df, experiment_path, 'Done.', 'prom train')\n",
    "        update_sheet(df, experiment_path, 'Done.', 'prom test')\n",
    "        update_sheet(df, experiment_path, removed_nan_reps, 'removed nan reps/session')\n",
    "        update_sheet(df, experiment_path, common_QC_channels.sum() , 'number of qc channels')\n",
    "        update_sheet(df, experiment_path, str([array for array, include in zip(combined_date_times, proc_session_mask_keep) if include]) , 'sessions kept')\n",
    "        \n",
    "        print(' Sessions kept: ', [array for array, include in zip(combined_date_times, proc_session_mask_keep) if include])\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Save experiment nwb file.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        print('... saving combined NWB Files.')\n",
    "\n",
    "        if combined_exists==False: \n",
    "            io = NWBHDF5IO(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom.nwb\"), \"w\") \n",
    "            io.write(combined_nwb)\n",
    "            io.close()\n",
    "            print(\"Combined file saved.\")\n",
    "        \n",
    "        if train_exists == False: \n",
    "            io = NWBHDF5IO(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom_train.nwb\"), \"w\") \n",
    "            io.write(combined_nwb_train)\n",
    "            io.close()\n",
    "            print(\"Train file saved.\")\n",
    "        \n",
    "        if test_exists==False:\n",
    "            io = NWBHDF5IO(os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}.prom_test.nwb\"), \"w\") \n",
    "            io.write(combined_nwb_test)\n",
    "            io.close()\n",
    "            print(\"Test file saved.\")\n",
    "\n",
    "        # display(combined_nwb)\n",
    "        # display(combined_nwb_train)\n",
    "        # display(combined_nwb_test)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Sheet 2\n",
    "xls = pd.ExcelFile(f'{os.path.dirname(cwd)}/pico_inventory.xlsx')\n",
    "sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "\n",
    "sheets['Sheet2'] = df  \n",
    "\n",
    "with pd.ExcelWriter(f'{os.path.dirname(cwd)}/pico_inventory.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "    for sheet_name, sheet_df in sheets.items():\n",
    "        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Validate the experiment nwb files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_file_paths = glob.glob(os.path.join(root_dir, '[exp]*', '*'))\n",
    "for experiment_path in experiment_file_paths:\n",
    "    if os.path.basename(experiment_path).startswith('exp'): \n",
    "        path = os.path.join(experiment_path, f\"{os.path.basename(experiment_path)}_combined.nwb\")\n",
    "        if os.path.isfile(path):\n",
    "            try:\n",
    "                    io = NWBHDF5IO(path, \"r\") \n",
    "                    nwbfile = io.read()\n",
    "                    display(nwbfile)\n",
    "                    io.close()\n",
    "                    break\n",
    "            except: print(f'This File can not be opened: {os.path.basename(experiment_path)}')\n",
    "            \n",
    "        else: print(f'No combined nwb found in: {os.path.basename(experiment_path)}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
