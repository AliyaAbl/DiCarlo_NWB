{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import h5py\n",
    "from brainio.assemblies import NeuronRecordingAssembly\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "from pynwb.base import Images\n",
    "from pynwb.image import RGBImage, ImageSeries\n",
    "import glob, os, yaml, pynwb\n",
    "import pytz  # This is required to handle timezone conversions\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os, glob, json\n",
    "import pandas as pd\n",
    "from pynwb.file import Subject\n",
    "import logging, sys, re\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import textwrap\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display as display_image\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "cwd = os.getcwd()\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "root_dir        = '/braintree/home/aliya277/inventory_new'\n",
    "df = pd.read_excel( os.path.dirname(cwd)+'/pico_inventory.xlsx' , sheet_name='Sheet2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sheet(df, exp_path, location, text):\n",
    "    imageset = os.path.basename(exp_path).split('.')[0].split('_')[1:]\n",
    "    if len(imageset) == 1: imageset = imageset[0]\n",
    "    elif len(imageset) > 1: imageset = '_'.join(imageset)\n",
    "    mask = df['ImageSet'] == imageset\n",
    "    index = df.index[mask].tolist()[0]\n",
    "    df.at[index, location] = text\n",
    "\n",
    "def extract_number(filename):\n",
    "    # Extract the number from the filename and return it as an integer\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "def update_prom_nwb(experiment_path, experiment_name, list_images_sorted, stimpath, count_1, check_image_order=True):\n",
    "\n",
    "    prom        = [x for x in os.listdir(experiment_path) if x.endswith('.prom.nwb')]\n",
    "    prom_test   = [x for x in os.listdir(experiment_path) if x.endswith('.prom_test.nwb')]\n",
    "    prom_train  = [x for x in os.listdir(experiment_path) if x.endswith('.prom_train.nwb')]\n",
    "\n",
    "    combined = False\n",
    "    train    = False\n",
    "    test     = False\n",
    "\n",
    "    StimuliIDs_train = None\n",
    "    StimuliIDs_test  = None\n",
    "\n",
    "    if len(prom) != 0: \n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Load combined nwb file.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        io = NWBHDF5IO(os.path.join(experiment_path, prom[0]), \"a\") \n",
    "        combined_nwb = io.read()\n",
    "        try: \n",
    "            combined_nwb.stimulus_template['StimulusSet']\n",
    "            print('Simulus Set already exists in combined.')\n",
    "            update_sheet(df, experiment_path, 'StimulusSet prom', 'Done')\n",
    "            # display(combined_nwb)\n",
    "            combined = True\n",
    "        except: pass\n",
    "\n",
    "        n_stimuli = []\n",
    "        for scratch in list(combined_nwb.scratch):\n",
    "            if scratch.startswith('PSTHs_QualityApproved'):\n",
    "                n_stimuli.append(combined_nwb.scratch[scratch][:].shape[0])\n",
    "        assert all(element == n_stimuli[0] for element in n_stimuli) == True, 'Number of Stimuli are not consistent over the PSTH!'\n",
    "        n_stimuli = n_stimuli[0]\n",
    "        # print(n_stimuli)\n",
    "        \n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Load combined_train nwb file.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        io_train = NWBHDF5IO(os.path.join(experiment_path, prom_train[0]), \"a\") \n",
    "        combined_nwb_train = io_train.read()\n",
    "        try: \n",
    "            combined_nwb_train.stimulus_template['StimulusSet']\n",
    "            print('Simulus Set already exists in train.')\n",
    "            update_sheet(df, experiment_path, 'StimulusSet prom train', 'Done')\n",
    "            # display(combined_nwb_train)\n",
    "            train = True\n",
    "        except: pass\n",
    "\n",
    "        n_stimuli_train = []\n",
    "        for scratch in list(combined_nwb_train.scratch):\n",
    "            if scratch.startswith('PSTHs_QualityApproved'):\n",
    "                n_stimuli_train.append(combined_nwb_train.scratch[scratch][:].shape[0])\n",
    "            if scratch.startswith('StimuliIDs'):\n",
    "                StimuliIDs_train = combined_nwb_train.scratch[scratch][:]\n",
    "        assert all(element == n_stimuli_train[0] for element in n_stimuli_train) == True, 'Number of Stimuli are not consistent over the PSTH!'\n",
    "        n_stimuli_train = n_stimuli_train[0]\n",
    "        # print(n_stimuli_train, len(StimuliIDs_train))\n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Load combined_test nwb file.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        io_test = NWBHDF5IO(os.path.join(experiment_path, prom_test[0]), \"a\") \n",
    "        combined_nwb_test = io_test.read()\n",
    "        try: \n",
    "            combined_nwb_test.stimulus_template['StimulusSet']\n",
    "            print('Simulus Set already exists in test.')\n",
    "            update_sheet(df, experiment_path, 'StimulusSet prom test', 'Done')\n",
    "            # display(combined_nwb_test)\n",
    "            test = True\n",
    "        except: pass\n",
    "\n",
    "        n_stimuli_test = []\n",
    "        for scratch in list(combined_nwb_test.scratch):\n",
    "            if scratch.startswith('PSTHs_QualityApproved'):\n",
    "                n_stimuli_test.append(combined_nwb_test.scratch[scratch][:].shape[0])\n",
    "            if scratch.startswith('StimuliIDs'):\n",
    "                StimuliIDs_test = combined_nwb_test.scratch[scratch][:]\n",
    "\n",
    "        assert all(element == n_stimuli_test[0] for element in n_stimuli_test) == True, 'Number of Stimuli are not consistent over the PSTH!'\n",
    "        n_stimuli_test = n_stimuli_test[0]\n",
    "        # print(n_stimuli_test, len(StimuliIDs_test))\n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Create StimulusSets.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        if combined == False or train == False or test == False: \n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # VideoStimulusSets\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            if stimpath == None:\n",
    "                external_file_prom = [os.path.join('../VideoStimulusSet', movie_path) for movie_path in list_images_sorted]\n",
    "\n",
    "                external_file_train = [external_file_prom[i] for i in StimuliIDs_train]\n",
    "                external_file_test  = [external_file_prom[i] for i in StimuliIDs_test]\n",
    "\n",
    "                print(external_file_prom)\n",
    "                print('------')\n",
    "                print(StimuliIDs_train)\n",
    "                print(external_file_train)\n",
    "                print(StimuliIDs_test)\n",
    "                print(external_file_test)\n",
    "\n",
    "                def StimulusSetDescription_test_train():\n",
    "                    return f\"This list references external files linking to movies that form the stimulus set, with each movie uniquely identified \\\n",
    "                    by a stimulus ID noted in the field 'starting_frame'. The 'StimuliIDs' array acts as a crucial link, mapping \\\n",
    "                    each PSTH entry to its corresponding movie within this set. If the PSTHs represent responses to 'n' \\\n",
    "                    different stimuli, the 'StimuliIDs' array will have 'n' entries, sequentially aligning each PSTH \\\n",
    "                    entry with its respective stimulus ID (e.g., the first PSTH entry is linked to the stimulus ID \\\n",
    "                    described in the first entry of the StimuliIDs' array). The movies in the stimulus set are \\\n",
    "                    organized in ascending order by their stimulus IDs. \\\n",
    "                    Note: The original function of 'starting_frame' is altered for this specific use case.\"\n",
    "            \n",
    "\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                # StimulusSet for combined.\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                StimulusMovieFile_combined = ImageSeries(\n",
    "                    name=\"StimulusSet\",\n",
    "                    description=f\"This list references external files linking to movies that form the stimulus set, with each movie uniquely identified \\\n",
    "                        by a stimulus ID noted in the field 'starting_frame'. Each movie has an associated 'starting_frame' field, repurposed to serve as the \\\n",
    "                        stimulusID in PSTHs. Filenames with sequential numbers starting from 0 align with these \\\n",
    "                        StimulusIDs. The arrangement ensures a one-to-one correspondence between \\\n",
    "                        the sequence of PSTH entries and the stimulus IDs; the first PSTH entry corresponds to the first stimulus ID, and so forth. \\\n",
    "                        The movies in the stimulus set are organized in ascending order by their stimulus ID. \\\n",
    "                        Note: The original function of 'starting_frame' is altered for this specific use case.\", \n",
    "                    unit=\"n.a.\",\n",
    "                    external_file=external_file_prom,\n",
    "                    format=\"external\", \n",
    "                    rate=0.0, \n",
    "                    starting_frame = np.linspace(0, n_stimuli-1, n_stimuli))\n",
    "\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                # StimulusSet for train.\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                StimulusMovieFile_train = ImageSeries(\n",
    "                    name=\"StimulusSet\",\n",
    "                    description=StimulusSetDescription_test_train(),\n",
    "                    unit=\"n.a.\",\n",
    "                    external_file=external_file_train,\n",
    "                    format=\"external\", \n",
    "                    rate=0.0, \n",
    "                    starting_frame = StimuliIDs_train)\n",
    "                \n",
    "                # ------------------------------------------------------------------------------ \n",
    "                # StimulusSet for test.\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                StimulusMovieFile_test = ImageSeries(\n",
    "                    name=\"StimulusSet\",\n",
    "                    description=StimulusSetDescription_test_train(),\n",
    "                    unit=\"n.a.\",\n",
    "                    external_file=external_file_test,\n",
    "                    format=\"external\", \n",
    "                    rate=0.0, \n",
    "                    starting_frame = StimuliIDs_test)\n",
    "                    \n",
    "                assert n_stimuli == len(external_file_prom), 'Number of Stimuli in prom does not match number of Images!'\n",
    "                assert n_stimuli_train == len(external_file_train), 'Number of Stimuli in prom train does not match number of Images!'\n",
    "                assert n_stimuli_test == len(external_file_test), 'Number of Stimuli in prom test does not match number of Images!'\n",
    "                \n",
    "                # ------------------------------------------------------------------------------ \n",
    "                # Append to nwb files.\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                if combined == False: \n",
    "                    try: \n",
    "                        combined_nwb.add_stimulus_template(timeseries=StimulusMovieFile_combined, use_sweep_table=False) \n",
    "                        print(f\"Added StimulusSet to combined nwb.\")\n",
    "                        display(combined_nwb)\n",
    "                        io.write(combined_nwb)\n",
    "                        io.close()  \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom', 'Done')\n",
    "                    except Exception as error: \n",
    "                        print(\"An error occurred:\", error) \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom', error)\n",
    "\n",
    "                if train == False:\n",
    "                    try: \n",
    "                        combined_nwb_train.add_stimulus_template(timeseries=StimulusMovieFile_train, use_sweep_table=False)  \n",
    "                        print(f\"Added StimulusSet to train nwb.\")\n",
    "                        display(combined_nwb_train)\n",
    "                        io_train.write(combined_nwb_train)\n",
    "                        io_train.close()    \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom train', 'Done')\n",
    "                    except Exception as error: \n",
    "                        print(\"An error occurred:\", error) \n",
    "                        io_train.close() \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom train', error)\n",
    "\n",
    "                if test == False:\n",
    "                    try: \n",
    "                        combined_nwb_test.add_stimulus_template(timeseries=StimulusMovieFile_test, use_sweep_table=False) \n",
    "                        print(f\"Added StimulusSet to test nwb.\")\n",
    "                        display(combined_nwb_test)\n",
    "                        io_test.write(combined_nwb_test)\n",
    "                        io_test.close()  \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom test', 'Done')\n",
    "                    except Exception as error: \n",
    "                        print(\"An error occurred:\", error)    \n",
    "                        io_test.close() \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom test', error)\n",
    "\n",
    "\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # Image StimulusSets\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            else:\n",
    "                \n",
    "                list_images         = []\n",
    "                list_images_train   = []\n",
    "                list_images_test    = []\n",
    "                \n",
    "                for temp, image in enumerate(list_images_sorted):\n",
    "                    image_counter = temp\n",
    "                    if count_1 ==1: image_counter = image_counter+1\n",
    "\n",
    "                    expected_image_name = f'im{image_counter}'\n",
    "                    if check_image_order:\n",
    "                        if image.split(\".\")[0] != expected_image_name and image.split(\".\")[0] !=f'{image_counter}' and image.split(\".\")[0] !=f'image{image_counter}' and image.split(\".\")[0] !=f'im{image_counter}_scrambled' and image.split(\".\")[0] !=f'{image_counter}':\n",
    "                                print(f'Image names do not increase with +1!!! Expected: {expected_image_name} or image{image_counter} or {image_counter}, Found: {image.split(\".\")[0]}')\n",
    "\n",
    "                    path = os.path.join(stimpath, image)\n",
    "                    img = Image.open(path)  # an example image\n",
    "                    \n",
    "                    image_file_name = f'exp_{experiment_name}_{temp}.png'\n",
    "\n",
    "                    if img.mode != 'RGB':\n",
    "                        data=np.array(img.convert(\"RGB\"))\n",
    "                    else: \n",
    "                        try:\n",
    "                            data=np.array(img)\n",
    "                        except: \n",
    "                            data_ = np.zeros(data.shape) #dummy image\n",
    "                            data = data_\n",
    "                            image_file_name = f'corrupted_png_{temp}.png'\n",
    "\n",
    "                    nwb_image = RGBImage(\n",
    "                        name= image_file_name,\n",
    "                        data = data,                            \n",
    "                        resolution=0.0,\n",
    "                        description= f\"StimulusID = {temp}\",\n",
    "                    )\n",
    "                    list_images.append(nwb_image)\n",
    "                \n",
    "                print(nwb_image.description, image)\n",
    "\n",
    "                list_images_train = [list_images[i] for i in StimuliIDs_train]\n",
    "                list_images_test  = [list_images[i] for i in StimuliIDs_test]\n",
    "                \n",
    "                assert n_stimuli == len(list_images), 'Number of Stimuli does not match number of Images!'\n",
    "\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                # Create nwb Images and append.\n",
    "                # ------------------------------------------------------------------------------ \n",
    "                def StimulusSetDescription_test_train():\n",
    "                    return f\"This list contains images that form the stimulus set, with each image uniquely identified \\\n",
    "                    by a stimulus ID noted in its description. The 'StimuliIDs' array acts as a crucial link, mapping \\\n",
    "                    each PSTH entry to its corresponding image within this set. If the PSTHs represent responses to 'n' \\\n",
    "                    different stimuli, the 'StimuliIDs' array will have 'n' entries, sequentially aligning each PSTH \\\n",
    "                    entry with its respective stimulus ID (e.g., the first PSTH entry is linked to the stimulus ID \\\n",
    "                    described in the first entry of the StimuliIDs' array). The images in the stimulus set are \\\n",
    "                    organized in ascending order by their stimulus IDs.\"\n",
    "\n",
    "                if combined == False: \n",
    "                    all_images = Images(\n",
    "                        name=f'StimulusSet',\n",
    "                        images= list_images ,\n",
    "                        description= f\"This list contains images that form the stimulus set, with each image uniquely identified \\\n",
    "                        by a stimulus ID noted in its description. The arrangement ensures a one-to-one correspondence between \\\n",
    "                        the sequence of PSTH entries and the stimulus IDs; the first PSTH entry corresponds to the first stimulus ID, and so forth. \\\n",
    "                        The images in the stimulus set are organized in ascending order by their stimulus ID.\")\n",
    "                    try: \n",
    "                        combined_nwb.add_stimulus_template(timeseries=all_images, use_sweep_table=False) \n",
    "                        print(f\"Added StimulusSet to combined nwb.\")\n",
    "                        # display(combined_nwb)\n",
    "                        io.write(combined_nwb)\n",
    "                        io.close()  \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom', 'Done')\n",
    "                    except Exception as error: \n",
    "                        print(\"An error occurred:\", error) \n",
    "                        io.close()  \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom', error)\n",
    "\n",
    "                if train == False:\n",
    "                    train_images = Images(\n",
    "                        name=f'StimulusSet',\n",
    "                        images= list_images_train ,\n",
    "                        description= StimulusSetDescription_test_train(),\n",
    "                    )\n",
    "                    try: \n",
    "                        combined_nwb_train.add_stimulus_template(timeseries=train_images, use_sweep_table=False)  \n",
    "                        print(f\"Added StimulusSet to train nwb.\")\n",
    "                        # display(combined_nwb_train)\n",
    "                        io_train.write(combined_nwb_train)\n",
    "                        io_train.close()    \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom train', 'Done')\n",
    "                    except Exception as error: \n",
    "                        print(\"An error occurred:\", error) \n",
    "                        io_train.close() \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom train', error)\n",
    "\n",
    "                if test == False:\n",
    "                    test_images = Images(\n",
    "                        name=f'StimulusSet',\n",
    "                        images= list_images_test ,\n",
    "                        description= StimulusSetDescription_test_train(),\n",
    "                    )\n",
    "                    try: \n",
    "                        combined_nwb_test.add_stimulus_template(timeseries=test_images, use_sweep_table=False) \n",
    "                        print(f\"Added StimulusSet to test nwb.\")\n",
    "                        # display(combined_nwb_test)\n",
    "                        io_test.write(combined_nwb_test)\n",
    "                        io_test.close()  \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom test', 'Done')\n",
    "                    except Exception as error: \n",
    "                        print(\"An error occurred:\", error)    \n",
    "                        io_test.close() \n",
    "                        update_sheet(df, experiment_path, 'StimulusSet prom test', error)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create per experiment stimulus set and add them to the per experiment nwb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ \n",
    "# Add Stimulus Sets to each prom nwb file going on BrainScore. This Cell needs to be run 3 times (due to nature of nwb_images)\n",
    "# ------------------------------------------------------------------------------ \n",
    "list_of_bs_exp_names = []\n",
    "df['StimulusSet prom'] = ''\n",
    "df['StimulusSet prom test'] = ''\n",
    "df['StimulusSet prom train'] = ''\n",
    "df['StimulusSetPath'] = ''\n",
    "for index, row in df.iterrows():\n",
    "    if row['BrainScore']=='Y': list_of_bs_exp_names.append(row['ImageSet'])\n",
    "    \n",
    "experiment_file_paths = glob.glob(os.path.join(root_dir, '[exp]*', '*'))\n",
    "stimulus_dir          = '/braintree/data2/active/users/sgouldin/experiments-codebase'\n",
    "stimuli_names         = os.listdir(stimulus_dir)\n",
    "\n",
    "for experiment_path in experiment_file_paths: \n",
    "    experiment_name =  \"_\".join(os.path.basename(experiment_path).split('.')[0].split('_')[1:])\n",
    "\n",
    "\n",
    "    # if not experiment_name.startswith('gratingsAdap_s'): continue    \n",
    "    \n",
    "    if experiment_name not in list_of_bs_exp_names: \n",
    "        continue \n",
    "\n",
    "    print('________________________________________________________________________________')\n",
    "    print(experiment_name)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Find Stimulus Directory name for each experiment. \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    if experiment_name == 'domain-transfer-2023':\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('domain_transfer') and not x.startswith('.')]\n",
    "    elif experiment_name == 'HVM-var6-2023':\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('HVM_var6') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('gratingsAdap_'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('gratingsAdap') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('gestalt'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('Gestalt') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('object_relations'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('ObjectRelationships') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('1_shapes'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('shapes') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('food'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('Food') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('shapenet360'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('ShapeNet360') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('shapegen'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('ShapeGens') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('sine_wave'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('sinewave_fullfield') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('square_sinewave'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('squarewave_fullfield') and not x.startswith('.')]\n",
    "    elif experiment_name.endswith('oasis900'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('Oasis900') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('oasis900_200'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('Oasis900') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('oasis100'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('OASIS100_control') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('oasis900rotated'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('OasisRotated') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('oasis900scrambled'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('OasisScramble') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('IAPS-200on'):\n",
    "        stim_name = [x for x in stimuli_names if x.endswith('IAPS') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('flicker'):\n",
    "        stim_name = [x for x in stimuli_names if x.startswith('flicker') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('ko_context_size'):\n",
    "        stim_name = [f'{x}/ko_context_size' for x in stimuli_names if x.startswith('old_rig1') and not x.startswith('.')]\n",
    "    elif experiment_name.startswith('muri'):\n",
    "        stim_name = [f'{x}/RSVP-MURI1320' for x in stimuli_names if x.startswith('old_rig1') and not x.startswith('.')]\n",
    "    else:\n",
    "        stim_name = [x for x in stimuli_names if x.endswith(experiment_name) and not x.startswith('.')]\n",
    "\n",
    "    if len(stim_name)==0: \n",
    "        print(f'    No Stim found: {experiment_name}') \n",
    "        update_sheet(df, experiment_path, 'StimulusSet prom', 'No StimulusSet found.')\n",
    "        continue\n",
    "\n",
    "    files_starting_with_vid = [file for file in os.listdir(os.path.join(stimulus_dir, stim_name[0])) if file.startswith('vid')]\n",
    "    ImageStimSetPath = None\n",
    "    VideoStimSetPath = None\n",
    "\n",
    "   \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Find 'images' folder in Stimulus Directory. (standard case)\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    if 'images' in os.listdir(os.path.join(stimulus_dir, stim_name[0])): \n",
    "        ImageStimSetPath = os.path.join(stimulus_dir, stim_name[0], 'images')\n",
    "    \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Find 'vid...' folder.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    elif len(files_starting_with_vid) > 0: \n",
    "        VideoStimSetPath = os.path.join(stimulus_dir, stim_name[0], files_starting_with_vid[0])\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Manually find folders for gratingsAdap.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    elif experiment_name.startswith('gratingsAdap_'):\n",
    "        season = experiment_name.split('_')[-1][-1]\n",
    "        file = f'season{season}'\n",
    "        list_videos = os.listdir(os.path.join(stimulus_dir, stim_name[0], file))\n",
    "        list_videos = [x for x in list_videos if not x.startswith('.')]\n",
    "        if list_videos[0].startswith('mv'): VideoStimSetPath = os.path.join(stimulus_dir, stim_name[0], file)\n",
    "        if list_videos[0].startswith('im'): ImageStimSetPath = os.path.join(stimulus_dir, stim_name[0], file)\n",
    "    \n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Manually find folders for object_relations.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    elif experiment_name.startswith('object_relations'):\n",
    "        files_starting_with_vid = [file for file in os.listdir(os.path.join(stimulus_dir, stim_name[0])) if file.startswith('mworks')]\n",
    "        VideoStimSetPath = os.path.join(stimulus_dir, stim_name[0], files_starting_with_vid[0])\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Manually find folders for oasis900.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    elif experiment_name =='oasis900' or experiment_name =='oasis900_200on':\n",
    "            ImageStimSetPath = os.path.join(stimulus_dir, stim_name[0], 'image_dicarlo_oasis900')\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Manually find folders for oasis100.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    elif experiment_name.startswith('oasis100'): \n",
    "        if experiment_name.endswith('c'): ImageStimSetPath = os.path.join(stimulus_dir, stim_name[0], 'images_control')\n",
    "        if experiment_name.endswith('o'): ImageStimSetPath = os.path.join(stimulus_dir, stim_name[0], 'images_original')\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Manually find folders for square_sinewave.\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    elif experiment_name == 'square_sinewave': \n",
    "        VideoStimSetPath = os.path.join(stimulus_dir, stim_name[0], 'squarewave_movies')\n",
    "\n",
    "    else: print(f'  No Images or Videos found for {experiment_name} {stim_name}')\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # For the following experiments, either SimulusSet is not found or the nwb files\n",
    "    # are not creted yet. Once both are done, check how the Simulus Direcories look \n",
    "    # and update this part (or the finding StimulusDirecory part.)\n",
    "    # ------------------------------------------------------------------------------ \n",
    "\n",
    "    # if experiment_name == 'NSD-COCO': \n",
    "    #     print(\"To Do\")\n",
    "    #     continue\n",
    "    if experiment_name == 'RF': \n",
    "        print(\"To Do\")\n",
    "        continue\n",
    "    elif experiment_name == 'flicker': \n",
    "        print(\"To Do\")\n",
    "        continue\n",
    "    elif experiment_name == 'gestalt': \n",
    "        print(\"To Do\")\n",
    "        continue\n",
    "    elif experiment_name.startswith('monkeyvalence'): \n",
    "        print(\"To Do\")\n",
    "        continue\n",
    "    elif experiment_name == 'sine_wave': \n",
    "        print(\"To Do\")\n",
    "        continue\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Create StimulusSet for each ImageStimulusSet\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    df_index = np.where(df['ImageSet'].to_numpy() == experiment_name)[0][0]\n",
    "\n",
    "    if ImageStimSetPath != None: \n",
    "        \n",
    "        check_image_order = True\n",
    "        count_1 = 0\n",
    "\n",
    "        if experiment_name =='oasis900' or experiment_name =='oasis900_200on':\n",
    "            def extract_integer(image_name):\n",
    "                try:\n",
    "                    return int(image_name[2:-4])\n",
    "                except: pass\n",
    "                \n",
    "            list_images = os.listdir(ImageStimSetPath)\n",
    "            list_images_sorted = [x for x in sorted(list_images, key = extract_number) if not x.startswith('.')]\n",
    "            csv_path = os.path.join('/', *ImageStimSetPath.split('/')[:-1], 'image_dicarlo_oasis900.csv')\n",
    "            df_csv = pd.read_csv(csv_path)\n",
    "            mapping = {extract_integer(row['image_file_name']): row['filename'] for index, row in df_csv.iterrows()}\n",
    "            reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "            list_images_sorted = sorted(list_images_sorted, key=lambda x: reverse_mapping.get(x, float('inf')))\n",
    "            check_image_order = False\n",
    "\n",
    "        elif experiment_name =='oasis900rotated':\n",
    "            \"\"\"\n",
    "            According to the mwel file, the stimulus is organized as first 900 ori and then 900 rot.\n",
    "            file_path = os.path.join('/', *path.split('/')[:-1],'image_set_definition_oriandrotated.mwel' ) \n",
    "            with open(file_path, 'r') as file:\n",
    "                mwel_content = file.read()\n",
    "            print(mwel_content)\n",
    "            \"\"\"\n",
    "            list_images = os.listdir(ImageStimSetPath)\n",
    "            list_images_sorted = [x for x in sorted(list_images, key = extract_number) if not x.startswith('.')]\n",
    "            list_images_sorted_ori = [x for x in list_images_sorted if x.startswith('im_ori')]\n",
    "            list_images_sorted_rot = [x for x in list_images_sorted if x.startswith('im_rot')]\n",
    "            list_images_sorted = list_images_sorted_ori + list_images_sorted_rot\n",
    "            check_image_order = False\n",
    "\n",
    "        elif experiment_name == 'oasis900scrambled' or experiment_name == 'oasis900scrambled_200on': \n",
    "            filename = 'image_set_definition_oriandscramble.mwel'\n",
    "            list_images_sorted = []\n",
    "            with open(os.path.join(stimulus_dir, stim_name[0], filename), 'r') as file:\n",
    "                mwel_content = file.read()\n",
    "            for substring in mwel_content.split('var imagefiles')[-1].split('\",\\n\"'):\n",
    "                if substring == 'images/im9_scrambled.jpg\"\\n]\\n': substring = 'images/im9_scrambled.jpg'\n",
    "                list_images_sorted.append(substring.split('/')[-1])\n",
    "            check_image_order = False\n",
    "\n",
    "        elif experiment_name == 'NSD-COCO': \n",
    "            list_images = os.listdir(ImageStimSetPath)\n",
    "            list_images_sorted = [x for x in sorted(list_images, key = extract_number) if not x.startswith('.')]\n",
    "            check_image_order = False\n",
    "\n",
    "        else:\n",
    "            list_images = os.listdir(ImageStimSetPath)\n",
    "            list_images_sorted = [x for x in sorted(list_images, key = extract_number) if not x.startswith('.')]\n",
    "\n",
    "        if experiment_name == 'shapenet360' or experiment_name =='1_shapes' or experiment_name =='food' or experiment_name =='shapegen_static': # Add when list_images_sorted start with 1 and not 0 in filename. \n",
    "            count_1 = 1\n",
    "            \n",
    "        print(experiment_name, ImageStimSetPath, list_images_sorted)\n",
    "        df.at[df_index, \"StimulusSetPath\"] = ImageStimSetPath\n",
    "        update_prom_nwb(experiment_path, experiment_name, list_images_sorted, ImageStimSetPath, count_1, check_image_order)\n",
    "\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    # Create StimulusSet for each VideoStimulusSet\n",
    "    # ------------------------------------------------------------------------------ \n",
    "    if VideoStimSetPath != None: \n",
    "\n",
    "        print(experiment_name, VideoStimSetPath)\n",
    "        df.at[df_index, \"StimulusSetPath\"] = VideoStimSetPath\n",
    "        list_movies = os.listdir(VideoStimSetPath)\n",
    "        list_movies_sorted = [x for x in sorted(list_movies, key = extract_number) if not x.startswith('.')]\n",
    "\n",
    "        if experiment_name == 'square_sinewave':\n",
    "            file_path = '/braintree/data2/active/users/sgouldin/experiments-codebase/squarewave_fullfield/movie_definition_squarewave_set1.mwel'\n",
    "            list_movies_sorted = []\n",
    "            with open(file_path, 'r') as file:\n",
    "                mwel_content = file.read()\n",
    "            for substring in mwel_content.split('var imagefiles')[-1].split('\",\\n\"'):\n",
    "                try: \n",
    "                    subsubstring = substring.split('\"')\n",
    "                    for sub in subsubstring:\n",
    "                        if sub.startswith('squarewave_movies'):\n",
    "                            list_movies_sorted.append(sub.split('/')[-1])\n",
    "                except: list_movies_sorted.append(substring.split('/')[-1])\n",
    "\n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Copy movies into experiment file.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        image_set_path = '/'.join(experiment_path.split('/')[:-1])\n",
    "        try: os.mkdir(os.path.join(image_set_path, 'VideoStimulusSet'))\n",
    "        except: pass\n",
    "        rename_flag = False\n",
    "        for movie in list_movies_sorted:\n",
    "            try: shutil.copy2(os.path.join(VideoStimSetPath, movie), os.path.join(image_set_path, 'VideoStimulusSet'))\n",
    "            except: rename_flag = True\n",
    "\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        # Update nwb files.\n",
    "        # ------------------------------------------------------------------------------ \n",
    "        list_movies_sorted_new = []\n",
    "        if rename_flag == False:\n",
    "            if experiment_name == 'motionset1' or experiment_name == 'moca' or experiment_name == 'afv'  or experiment_name == 'faceemovids' or experiment_name.startswith('gratingsAdap_'):\n",
    "                for movie in list_movies_sorted:\n",
    "                    match = re.search(r'\\d+', movie)\n",
    "                    if match:\n",
    "                        number = int(match.group())\n",
    "                        movie_filename_new = f'exp_{experiment_name}_{number}.mp4'\n",
    "                        list_movies_sorted_new.append(movie_filename_new)\n",
    "                        moviepath = os.path.join(image_set_path, 'VideoStimulusSet')\n",
    "                        os.rename(os.path.join(moviepath, movie), os.path.join(moviepath, movie_filename_new))\n",
    "\n",
    "                # update_exp_nwb_movies(experiment_path, list_movies_sorted, 0, check_image_order=True)\n",
    "            elif experiment_name == 'gestalt' or experiment_name == 'Co3D':\n",
    "                for movie in list_movies_sorted:\n",
    "                    match = re.search(r'\\d+', movie)\n",
    "                    if match:\n",
    "                        number = int(match.group())\n",
    "                        movie_filename_new = f'exp_{experiment_name}_{number-1}.mp4'\n",
    "                        list_movies_sorted_new.append(movie_filename_new)\n",
    "                        moviepath = os.path.join(image_set_path, 'VideoStimulusSet')\n",
    "                        os.rename(os.path.join(moviepath, movie), os.path.join(moviepath, movie_filename_new))\n",
    "                # update_exp_nwb_movies(experiment_path, list_movies_sorted, 1, check_image_order=True)\n",
    "            elif experiment_name == 'square_sinewave' or experiment_name == 'object_relations':\n",
    "                for movie, number in zip(list_movies_sorted, range(len(list_movies_sorted))):\n",
    "                    movie_filename_new = f'exp_{experiment_name}_{number}.mp4'\n",
    "                    list_movies_sorted_new.append(movie_filename_new)\n",
    "                    moviepath = os.path.join(image_set_path, 'VideoStimulusSet')\n",
    "                    os.rename(os.path.join(moviepath, movie), os.path.join(moviepath, movie_filename_new))\n",
    "            else: list_movies_sorted_new = list_movies_sorted\n",
    "\n",
    "        else: list_movies_sorted_new = list_movies_sorted\n",
    "        \n",
    "        print(list_movies_sorted_new)\n",
    "        update_prom_nwb(experiment_path, experiment_name, list_movies_sorted_new, None, 0, check_image_order=True)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ \n",
    "# Update Sheet 2 of Excel File.\n",
    "# ------------------------------------------------------------------------------ \n",
    "xls = pd.ExcelFile(f'{os.path.dirname(cwd)}/pico_inventory.xlsx')\n",
    "sheets = {sheet: xls.parse(sheet) for sheet in xls.sheet_names}\n",
    "\n",
    "sheets['Sheet2'] = df  \n",
    "\n",
    "with pd.ExcelWriter(f'{os.path.dirname(cwd)}/pico_inventory.xlsx', engine='openpyxl', mode='w') as writer:\n",
    "    for sheet_name, sheet_df in sheets.items():\n",
    "        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------ \n",
    "# Basic Checks for prom files. \n",
    "# ------------------------------------------------------------------------------ \n",
    "\n",
    "def prettyprint(string):\n",
    "    pretty_string = ' '.join(string.split())\n",
    "    wrapped_string = textwrap.fill(pretty_string, width=120)\n",
    "    return wrapped_string\n",
    "\n",
    "def get_image_hash(img):\n",
    "    # Convert the image to bytes\n",
    "    img_bytes = img.tobytes()\n",
    "    # Use hashlib to generate a hash\n",
    "    hash = hashlib.md5(img_bytes).hexdigest()\n",
    "    return hash\n",
    "\n",
    "def are_images_identical(img1, img2):\n",
    "    hash1 = get_image_hash(img1)\n",
    "    hash2 = get_image_hash(img2)\n",
    "    return hash1 == hash2\n",
    "\n",
    "def check_prom_file(experiment_path):\n",
    "    prom        = [x for x in os.listdir(experiment_path) if x.endswith('.prom.nwb')]\n",
    "    prom_test   = [x for x in os.listdir(experiment_path) if x.endswith('.prom_test.nwb')]\n",
    "    prom_train  = [x for x in os.listdir(experiment_path) if x.endswith('.prom_train.nwb')]\n",
    "    \n",
    "    if len(prom) != 0: \n",
    "        files = [prom[0], prom_train[0], prom_test[0]]\n",
    "        for file in files:\n",
    "            if not file.startswith('exp_gratingsAdap_s'): continue \n",
    "            print('________________________________________', os.path.basename(experiment_path), file)\n",
    "            \n",
    "            io = NWBHDF5IO(os.path.join(experiment_path, file), \"r\") \n",
    "            prom_nwb = io.read()\n",
    "            \n",
    "            assert len(prom_nwb.keywords[:]) != 0, 'No keywords in file.'\n",
    "            if len(prom_nwb.notes) == 0: print('No notes in file.')\n",
    "\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # Check PSTH shapes.\n",
    "            # ------------------------------------------------------------------------------ \n",
    "        \n",
    "            # print(prettyprint(prom_nwb.scratch['PSTHs_Normalizers_SessionMerged'].description))\n",
    "            # print('__________')\n",
    "            # print(prettyprint(prom_nwb.scratch['PSTHs_QualityApproved_SessionMerged'].description))\n",
    "            # print('__________')\n",
    "            # print(prettyprint(prom_nwb.scratch['PSTHs_QualityApproved_ZScored_SessionMerged'].description))\n",
    "            # print('__________')\n",
    "            # print(prettyprint(prom_nwb.scratch['QualityApprovedChannelMasks'].description))\n",
    "\n",
    "\n",
    "            psth_zscored = prom_nwb.scratch['PSTHs_QualityApproved_ZScored_SessionMerged'][:]\n",
    "            psth         = prom_nwb.scratch['PSTHs_QualityApproved_SessionMerged'][:]\n",
    "            n_stimuli = psth.shape[0]\n",
    "\n",
    "            assert len(psth_zscored.shape) == 4, 'wrong shape for PSTHs_QualityApproved_ZScored_SessionMerged '\n",
    "            assert len(psth.shape) == 4,         'wrong shape for PSTHs_QualityApproved_SessionMerged ' \n",
    "            assert psth_zscored.shape == psth.shape, 'PSTH Shapes different.'\n",
    "            assert np.min(psth_zscored) != 0, 'ZScored_PSTH not Z-scored'\n",
    "            assert np.min(psth) == 0, 'PSTH containing negative numbers.'\n",
    "\n",
    "            # ------------------------------------------------------------------------------ \n",
    "            # Check Stimulus Set.\n",
    "            # ------------------------------------------------------------------------------ \n",
    "\n",
    "            # print((prettyprint(prom_nwb.stimulus_template['StimulusSet'].description)))\n",
    "            imageset = os.path.basename(experiment_path).split('.')[0]\n",
    "            try: \n",
    "                stimulus_path = df[df['ImageSet'] == imageset.replace(\"exp_\", \"\", 1)]['StimulusSetPath'].tolist()[0]\n",
    "                print(stimulus_path)\n",
    "                list_images_sorted = [x for x in sorted(os.listdir(stimulus_path), key = extract_number) if not x.startswith('.')]\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    list_images_nwb = sorted(list(prom_nwb.stimulus_template['StimulusSet'].images.keys()), key = lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "                    stimulus_ids = [int(filename.split('_')[-1].split('.')[0]) for filename in list_images_nwb]\n",
    "\n",
    "                    if file.split('.')[-2] != 'prom':\n",
    "                        stimulus_ids_nwb = list(prom_nwb.scratch['StimuliIDs'][:])\n",
    "                        are_lists_equal = sorted(stimulus_ids) == sorted(stimulus_ids_nwb)\n",
    "                        if are_lists_equal == False: print('Images and StimuluIDs are not identical!')\n",
    "                        if len(stimulus_ids_nwb) != n_stimuli: print('Number of Stimuli do not match with StimulusIDs and n_stimuli')\n",
    "\n",
    "                    n_images = len(prom_nwb.stimulus_template['StimulusSet'].images)\n",
    "                    image_id = random.randint(0, n_images)\n",
    "\n",
    "                    for image_id in range(n_images):\n",
    "                        try:\n",
    "                            image = prom_nwb.stimulus_template['StimulusSet'].images[f'{imageset}_{image_id}.png'][:]\n",
    "                            im = Image.fromarray(image)\n",
    "                            img = Image.open(os.path.join(stimulus_path, list_images_sorted[image_id]))\n",
    "                            img_= np.array(img)\n",
    "                            img = im = Image.fromarray(img_)\n",
    "\n",
    "                            if are_images_identical(im, img) == False: \n",
    "                                display_image(img)\n",
    "                                display_image(im)\n",
    "                                print(f'Image {image_id} are not Identical for file {list_images_sorted[image_id]}!')\n",
    "                                print(stimulus_path)\n",
    "                                print(list_images_sorted)\n",
    "                                # break\n",
    "\n",
    "                        except: pass #Exception as e: print('Image not in this sub-set.')\n",
    "\n",
    "\n",
    "                    pattern = re.compile(fr'{imageset}_\\d+\\.png')\n",
    "                    all_match_pattern = all(pattern.match(filename) for filename in list_images_nwb)\n",
    "                    if all_match_pattern == False: \n",
    "                        print(' NWB Image Names are not consistent. At least one image in source is corrupted.')\n",
    "\n",
    "                except: \n",
    "\n",
    "                    if file.split('.')[-2] != 'prom':\n",
    "                        stimulus_ids_nwb = (prom_nwb.stimulus_template['StimulusSet'].starting_frame)\n",
    "                        if len(stimulus_ids_nwb) != n_stimuli: print('Number of Stimuli do not match with StimulusIDs and n_stimuli')\n",
    "            except: print('No StimulusPath.')\n",
    "            io.close()\n",
    "\n",
    "\n",
    "experiment_file_paths = glob.glob(os.path.join(root_dir, '[exp]*', '*'))\n",
    "stimulus_dir          = '/braintree/data2/active/users/sgouldin/experiments-codebase'\n",
    "stimuli_names         = os.listdir(stimulus_dir)\n",
    "\n",
    "\n",
    "for experiment_path in experiment_file_paths: \n",
    "    check_prom_file(experiment_path)\n",
    "    # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
