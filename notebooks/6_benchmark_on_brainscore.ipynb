{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark on BrainScore using nwb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pynwb import NWBHDF5IO, NWBFile\n",
    "from pynwb.base import Images\n",
    "from pynwb.image import RGBImage, ImageSeries\n",
    "from brainio.stimuli import StimulusSet\n",
    "from PIL import Image\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import xarray as xr\n",
    "from brainio.assemblies import NeuronRecordingAssembly, NeuroidAssembly\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "from brainscore_vision import load_benchmark\n",
    "from brainio.packaging import package_stimulus_set, package_data_assembly\n",
    "from brainscore_vision import load_benchmark, load_dataset, load_ceiling, load_model, load_metric\n",
    "import brainio\n",
    "from brainscore_vision.metric_helpers.transformations import CrossValidation\n",
    "from brainscore_vision.benchmark_helpers.neural_common import NeuralBenchmark, average_repetition\n",
    "from brainscore_vision.utils import LazyLoad\n",
    "\n",
    "\n",
    "def extract_number(filename):\n",
    "    # Extract the number from the filename and return it as an integer\n",
    "    match = re.search(r'\\d+', filename.split('_')[-1])\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "def get_stimuli(nwb_file, experiment_path, exp_name):\n",
    "    image_paths = []\n",
    "    stimuli     = []\n",
    "    image_ids   = [int(x.split('_')[-1].split('.png')[0]) for x in sorted(list(nwb_file.stimulus_template[f'StimulusSet'].images), key = extract_number)]\n",
    "    stimulus_id = 0\n",
    "    \n",
    "    try: os.mkdir(os.path.join(experiment_path, 'images'))\n",
    "    except: pass \n",
    "\n",
    "    print(\"Iterating over the images ...\")\n",
    "    for i in image_ids:\n",
    "        try:\n",
    "            image = nwb_file.stimulus_template[f'StimulusSet'][f'exp_{exp_name}_{i}.png'][:]\n",
    "            im = Image.fromarray(image)\n",
    "            if not os.path.isfile(os.path.join( experiment_path, 'images', f'exp_{exp_name}_{i}.png')):\n",
    "                im.save(os.path.join( experiment_path, 'images', f'exp_{exp_name}_{i}.png'))\n",
    "            image_paths.append(os.path.join( experiment_path, 'images', f'exp_{exp_name}_{i}.png'))\n",
    "        \n",
    "            stimuli.append({\n",
    "                'stimulus_id': stimulus_id,\n",
    "                'image_id': stimulus_id,\n",
    "                'id': stimulus_id,\n",
    "                # 'stimulus_path_within_store': f\"exp_{exp_name}_{i}\",\n",
    "                # 'stimulus_set': exp_name,\n",
    "                'image_number': i,\n",
    "                # 'stimulus_nwb_file_path': f\"{os.path.join(*experiment_path.split('/')[:-1])}/stimulus_template/StimulusSet/exp_{exp_name}_{i}.png\",\n",
    "                'image_file_name': f'exp_{exp_name}_{i}.png',\n",
    "                'filename': f'exp_{exp_name}_{i}.png',\n",
    "                'background_id':'',\n",
    "                's':'',\t\n",
    "                'rxy':'',\n",
    "                'tz':'',\t\n",
    "                'category_name':'',\n",
    "                'rxz_semantic':'',\t\n",
    "                'ty':'',\n",
    "                'ryz':'',\n",
    "                'object_name':'',\t\n",
    "                'variation':'',\t\n",
    "                'size':'',\t\n",
    "                'rxy_semantic':'',\n",
    "                'ryz_semantic':'',\n",
    "                'rxz':''\t                \n",
    "            })\n",
    "            stimulus_id += 1\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "\n",
    "    stimuli = pd.DataFrame(stimuli)   \n",
    "    return stimuli, image_paths\n",
    "\n",
    "def filter_neuroids(assembly, threshold):\n",
    "    ceiler = load_ceiling('internal_consistency')\n",
    "    ceiling = ceiler(assembly)\n",
    "    ceiling = ceiling.raw\n",
    "    ceiling = CrossValidation().aggregate(ceiling)\n",
    "    pass_threshold = ceiling >= threshold\n",
    "    assembly = assembly[{'neuroid': pass_threshold}]\n",
    "    return assembly\n",
    "\n",
    "def load_assembly(assembly, average_repetitions, region):\n",
    "    assembly = assembly.sel(region=region)\n",
    "    assembly['region'] = 'neuroid', [region] * len(assembly['neuroid'])\n",
    "    assembly.load()\n",
    "    assembly = assembly.sel(time_bin_id=0)  # 70-170ms\n",
    "    assembly = assembly.squeeze('time_bin')\n",
    "    assert NUMBER_OF_TRIALS == len(np.unique(assembly.coords['repetition']))\n",
    "    assert VISUAL_DEGREES == assembly.attrs['image_size_degree']\n",
    "    if average_repetitions:\n",
    "        assembly = average_repetition(assembly)\n",
    "    return assembly\n",
    "\n",
    "def _NeuralBenchmark(assembly_, region):\n",
    "    # print('load rep')\n",
    "    assembly_repetition = LazyLoad(lambda region=region: load_assembly(assembly_, average_repetitions=False, region=region))\n",
    "    # print('load assy')\n",
    "    assembly = LazyLoad(lambda region=region: load_assembly(assembly_, average_repetitions=True, region=region))\n",
    "    # metric = load_metric('pls', crossvalidation_kwargs=dict(stratification_coord='object_name'))\n",
    "    # print('load metric')\n",
    "    metric = load_metric('pls')\n",
    "    # print('load ceilling')\n",
    "    ceiler = load_ceiling('internal_consistency')\n",
    "    # print('return nb')\n",
    "    return NeuralBenchmark(identifier=f'Aliya2024.{exp_name}.{region}-pls', version=1,\n",
    "                           assembly=assembly, similarity_metric=metric,\n",
    "                           visual_degrees=VISUAL_DEGREES, number_of_trials=NUMBER_OF_TRIALS,\n",
    "                           ceiling_func=lambda: ceiler(assembly_repetition),\n",
    "                           parent=region)\n",
    "\n",
    "def get_meta(nwb_file):\n",
    "    s = (nwb_file.scratch['PSTHs_QualityApproved_ZScored_SessionMerged'].description.split('[start_time_ms, stop_time_ms, tb_ms]: ')[-1])\n",
    "    numbers = s.strip('[]').split()\n",
    "    array = np.array(numbers, dtype=int)\n",
    "    return array\n",
    "\n",
    "def get_neuroids(nwb_file):\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    # Get electrode metadata from nwb file.\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    data_list = []  \n",
    "    for i in range(len(nwb_file.electrodes['location'])):\n",
    "        data_dict       = {}\n",
    "        location_item   = nwb_file.electrodes['location'][i]\n",
    "        group_item      = nwb_file.electrodes['group'][i] \n",
    "        bank_item       = nwb_file.electrodes['group_name'][i] \n",
    "        label_item      = nwb_file.electrodes['label'][i]\n",
    "        try: label_item = int(nwb_file.electrodes['label'][i].split('_')[0])\n",
    "        except: label_item = nwb_file.electrodes['label'][i]\n",
    "\n",
    "        location_match = re.search(r'\\[(\\d+).0, (\\d+).0, (\\d+).0\\]', location_item)\n",
    "        if location_match:\n",
    "            data_dict['col'] = location_match.group(2)\n",
    "            data_dict['row'] = location_match.group(1)\n",
    "            data_dict['elec'] = location_match.group(3) \n",
    "\n",
    "        serialnumer = group_item.description.split('Serialnumber: ')[-1]\n",
    "        data_dict['arr'] = serialnumer\n",
    "\n",
    "        group_match = re.search(r\"\\['(\\w+)', '(\\w+)', '(\\w+)'\\]\", group_item.location)\n",
    "        if group_match:\n",
    "            data_dict['hemisphere']  = group_match.group(1)\n",
    "            data_dict['region']  = group_match.group(2)\n",
    "            data_dict['subregion'] = group_match.group(3)\n",
    "        \n",
    "        data_dict['bank']  = bank_item.split('_')[-1]\n",
    "        data_dict['animal'] = subject\n",
    "        if (label_item) < 10:\n",
    "            neuroid_id = f\"{bank_item.split('_')[-1]}-00{label_item}\"\n",
    "            elec = f\"00{label_item}\"\n",
    "        else:\n",
    "            neuroid_id = f\"{bank_item.split('_')[-1]}-0{label_item}\"\n",
    "            elec = f\"00{label_item}\"\n",
    "        data_dict['neuroid_id']  = neuroid_id\n",
    "        data_dict['elec']  = elec\n",
    "\n",
    "        data_list.append(data_dict)\n",
    "    \n",
    "    neuroid_meta = pd.DataFrame(data_list)\n",
    "    return neuroid_meta\n",
    "\n",
    "def get_QC_neurids(nwb_file):\n",
    "    '''\n",
    "    This Method uses logical OR to find the common QC channels. (Closer to the BrainScore Method)\n",
    "    '''\n",
    "    psth = nwb_file.scratch['PSTHs_ZScored_SessionMerged'][:]\n",
    "    common_QC_channels = np.logical_and.reduce(nwb_file.scratch['QualityApprovedChannelMasks'])\n",
    "    channel_masks_day = nwb_file.scratch['QualityApprovedChannelMasks'][:]\n",
    "    channel_mask_all_list = []\n",
    "    day = 0\n",
    "    for key in sorted(nwb_file.scratch.keys()):\n",
    "        if key.startswith('PSTHs_QualityApproved_20'):\n",
    "            # print(key)\n",
    "            nreps_per_day = nwb_file.scratch[key][:].shape[1]\n",
    "            for i in range(nreps_per_day):\n",
    "                channel_mask_all_list.append(channel_masks_day[day,:])\n",
    "            day += 1\n",
    "    channel_mask_all = np.array(channel_mask_all_list)\n",
    "\n",
    "    assert channel_mask_all.shape[0] == psth.shape[1]\n",
    "    filtered_neurids = np.any(channel_mask_all, axis=0)\n",
    "    return filtered_neurids, common_QC_channels\n",
    "\n",
    "def load_responses(nwb_file, stimuli, use_QC_data = True, do_filter_neuroids = False, use_brainscore_filter_neuroids_method=False):\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    # Get the PSTH and normalizer PSTH\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    normalizer_psth = nwb_file.scratch['PSTHs_Normalizers_SessionMerged'][:]\n",
    "    if not use_QC_data:\n",
    "        psth            = nwb_file.scratch['PSTHs_ZScored_SessionMerged'][:]\n",
    "    elif use_QC_data:\n",
    "        psth            = nwb_file.scratch['PSTHs_QualityApproved_ZScored_SessionMerged'][:]\n",
    "    meta = get_meta(nwb_file)\n",
    "    qc_array_or, qc_array_and = get_QC_neurids(nwb_file)\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    # Compute firing rates.\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    timebins = [[70, 170], [170, 270], [50, 100], [100, 150], [150, 200], [200, 250], [70, 270]]\n",
    "    timebase = np.arange(meta[0], meta[1], meta[2])\n",
    "    assert len(timebase) == psth.shape[2]\n",
    "    rate = np.empty((len(timebins), psth.shape[0], psth.shape[1], psth.shape[3]))\n",
    "    for idx, tb in enumerate(timebins):\n",
    "        t_cols = np.where((timebase >= (tb[0])) & (timebase < (tb[1])))[0]\n",
    "        rate[idx] = np.mean(psth[:, :, t_cols, :], axis=2)  # Shaped time bins x images x repetitions x channels\n",
    "\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    # Load neuroid metadata and image metadata\n",
    "    #-----------------------------------------------------------------------------------------------------------------------------\n",
    "    image_id     = stimuli.image_number\n",
    "    neuroid_meta = get_neuroids(nwb_file)\n",
    "\n",
    "    assembly = xr.DataArray(rate,\n",
    "                            coords={'repetition': ('repetition', list(range(rate.shape[2]))),\n",
    "                                    'time_bin_id': ('time_bin', list(range(rate.shape[0]))),\n",
    "                                    'time_bin_start': ('time_bin', [x[0] for x in timebins]),\n",
    "                                    'time_bin_stop': ('time_bin', [x[1] for x in timebins]),\n",
    "                                    'image_id': ('image', image_id)},\n",
    "                            dims=['time_bin', 'image', 'repetition', 'neuroid'])\n",
    "    if use_QC_data:\n",
    "        for column_name, column_data in neuroid_meta.iteritems():\n",
    "            assembly = assembly.assign_coords(**{f'{column_name}': ('neuroid', list(column_data.values[qc_array_and]))})\n",
    "    else:\n",
    "        for column_name, column_data in neuroid_meta.iteritems():\n",
    "            assembly = assembly.assign_coords(**{f'{column_name}': ('neuroid', list(column_data.values))})\n",
    "\n",
    "    assembly = assembly.sortby(assembly.image_id)\n",
    "    stimuli  = stimuli.sort_values(by='image_id').reset_index(drop=True)\n",
    "    for column_name, column_data in stimuli.iteritems():\n",
    "        assembly = assembly.assign_coords(**{f'{column_name}': ('image', list(column_data.values))})\n",
    "    assembly = assembly.sortby(assembly.id)  \n",
    "\n",
    "    # Collapse dimensions 'image' and 'repetitions' into a single 'presentation' dimension\n",
    "    assembly = assembly.stack(presentation=('image', 'repetition')).reset_index('presentation')\n",
    "    assembly = NeuronRecordingAssembly(assembly)\n",
    "\n",
    "    if do_filter_neuroids and use_brainscore_filter_neuroids_method:\n",
    "        # Filter noisy electrodes\n",
    "        psth = normalizer_psth\n",
    "        if psth.shape[0] == 26:\n",
    "            psth = psth[:-1,:,:,:] #remove grey image\n",
    "        t_cols = np.where((timebase >= (70 )) & (timebase < (170)))[0]\n",
    "        rate = np.mean(psth[:, :, t_cols, :], axis=2)\n",
    "        normalizer_assembly = xr.DataArray(rate,\n",
    "                                        coords={'repetition': ('repetition', list(range(rate.shape[1]))),\n",
    "                                                'image_id': ('image', list(range(rate.shape[0]))),\n",
    "                                                'id': ('image', list(range(rate.shape[0])))},\n",
    "                                        dims=['image', 'repetition', 'neuroid'])\n",
    "        for column_name, column_data in neuroid_meta.iteritems():\n",
    "            normalizer_assembly = normalizer_assembly.assign_coords(\n",
    "                **{f'{column_name}': ('neuroid', list(column_data.values))})\n",
    "\n",
    "        normalizer_assembly = normalizer_assembly.assign_coords(**{f'{\"stimulus_id\"}': ('image', list(np.linspace(1,psth.shape[0],psth.shape[0], dtype=int)))})# had to add this part TODO: remove the last grey image from normalizer set when doing the nwb conversion\n",
    "        normalizer_assembly = normalizer_assembly.stack(presentation=('image', 'repetition')).reset_index('presentation')\n",
    "        normalizer_assembly = normalizer_assembly.drop('image')\n",
    "        normalizer_assembly = normalizer_assembly.transpose('presentation', 'neuroid')\n",
    "        normalizer_assembly = NeuronRecordingAssembly(normalizer_assembly)\n",
    "\n",
    "        filtered_assembly = filter_neuroids(normalizer_assembly, 0.7)\n",
    "        assembly = assembly.sel(neuroid=np.isin(assembly.neuroid_id, filtered_assembly.neuroid_id))\n",
    "\n",
    "    elif do_filter_neuroids and not use_brainscore_filter_neuroids_method:\n",
    "        filter_assembly = xr.DataArray(qc_array_or,\n",
    "                                dims=['neuroid'])\n",
    "        for column_name, column_data in neuroid_meta.iteritems():\n",
    "            filter_assembly = filter_assembly.assign_coords(\n",
    "                **{f'{column_name}': ('neuroid', list(column_data.values))})\n",
    "            \n",
    "        filtered_assembly = filter_assembly.sel(neuroid=qc_array_or)\n",
    "        assembly = assembly.sel(neuroid=np.isin(assembly.neuroid_id, filtered_assembly.neuroid_id))\n",
    "\n",
    "    elif use_QC_data:\n",
    "        filter_assembly = xr.DataArray(qc_array_and,\n",
    "                                dims=['neuroid'])\n",
    "        for column_name, column_data in neuroid_meta.iteritems():\n",
    "            filter_assembly = filter_assembly.assign_coords(\n",
    "                **{f'{column_name}': ('neuroid', list(column_data.values))})\n",
    "            \n",
    "        filtered_assembly = filter_assembly.sel(neuroid=qc_array_and)\n",
    "        assembly = assembly.sel(neuroid=np.isin(assembly.neuroid_id, filtered_assembly.neuroid_id))\n",
    "\n",
    "    assembly = assembly.transpose('presentation', 'neuroid', 'time_bin')\n",
    "\n",
    "    # Add other experiment related info\n",
    "    assembly.attrs['image_size_degree'] = 8\n",
    "    assembly.attrs['stim_on_time_ms'] = 100\n",
    "\n",
    "    return assembly\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "# Load NWB file and get stimuli.\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "exp_name    = 'domain-transfer-2023'\n",
    "subject     = 'pico'\n",
    "nwb_file_path   = '/braintree/home/aliya277/inventory_new/exp_{}/exp_{}.sub_{}/exp_{}.sub_{}.prom.nwb'.format(exp_name, exp_name, subject, exp_name, subject)\n",
    "experiment_path = '/braintree/home/aliya277/inventory_new/exp_{}/exp_{}.sub_{}'.format(exp_name, exp_name, subject)\n",
    "\n",
    "print(f'DATASET: {exp_name}')\n",
    "### Load nwb file\n",
    "print(\"Loading the NWB file ...\")\n",
    "io = NWBHDF5IO(nwb_file_path, \"r\") \n",
    "nwb_file = io.read()    \n",
    "\n",
    "print(\"Getting my Stimulus Set ...\")\n",
    "aliya_stimuli, aiya_imagepaths = get_stimuli(nwb_file, experiment_path, exp_name)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# Creata Assemblies.\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Creating Assemblies ...\")\n",
    "\n",
    "# This assembly is created using the QC method used to create the QualitApproved SessionMerged PSTH from the NWB files. This method is quite strict.\n",
    "aliya_assembly1  = load_responses(nwb_file, aliya_stimuli)\n",
    "# This assembly is created using a slightly more loose QC method using the p-values from the NWB files. This method is more similar to the brainscore method.\n",
    "aliya_assembly2  = load_responses(nwb_file, aliya_stimuli, use_QC_data = False, do_filter_neuroids = True,  use_brainscore_filter_neuroids_method=False)\n",
    "# This assembly is created using the brainscore QC method.\n",
    "aliya_assembly3  = load_responses(nwb_file, aliya_stimuli, use_QC_data = False, do_filter_neuroids = True,  use_brainscore_filter_neuroids_method=True)\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "# Creata Benchmarks.\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "print(\"Creating Benchmarks ...\")\n",
    "\n",
    "VISUAL_DEGREES = 8\n",
    "psth = nwb_file.scratch['PSTHs_ZScored_SessionMerged'][:]\n",
    "NUMBER_OF_TRIALS = psth.shape[1]\n",
    "\n",
    "def update_assembly(assembly):\n",
    "    assembly.name = f'dicarlo.{exp_name}.Aliya2024'\n",
    "    stimuli = StimulusSet(aliya_stimuli)\n",
    "    stimuli.stimulus_paths = aiya_imagepaths\n",
    "    stimuli.name = f\"Aliya2024_{exp_name}\"\n",
    "    stimuli.identifier = f\"Aliya2024_{exp_name}\"\n",
    "    assembly.attrs['stimulus_set'] = stimuli\n",
    "    return assembly\n",
    "\n",
    "aliya2024_benchmark1 = _NeuralBenchmark( update_assembly(aliya_assembly1), 'IT')\n",
    "aliya2024_benchmark2 = _NeuralBenchmark( update_assembly(aliya_assembly2), 'IT')\n",
    "aliya2024_benchmark3 = _NeuralBenchmark( update_assembly(aliya_assembly3), 'IT')\n",
    "io.close()\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "# Score Benchmarks.\n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "print('Scoring Models ...')\n",
    "list_models=['pixels', 'tv_efficientnet-b1', 'alexnet', 'resnet50_julios', 'yudixie_resnet50_imagenet1kpret_0_240312', 'eBarlow_Vanilla', 'eMMCR_Vanilla']\n",
    "for model_ in list_models:\n",
    "    try:\n",
    "        print('------------------------------------------')\n",
    "        print('------------------------------------------')\n",
    "        print('Scoring Model: ',model_)\n",
    "        model = load_model(model_)\n",
    "        score_1_aliya = aliya2024_benchmark1(model)\n",
    "        score_2_aliya = aliya2024_benchmark2(model)\n",
    "        score_3_aliya = aliya2024_benchmark3(model)\n",
    "        print('\\n------------------------------------------')\n",
    "        print('Score1:')\n",
    "        print(score_1_aliya)\n",
    "        print('\\n------------------------------------------')\n",
    "        print('Score2:')\n",
    "        print(score_2_aliya)\n",
    "        print('\\n------------------------------------------')\n",
    "        print('Score3:')\n",
    "        print(score_3_aliya)\n",
    "        print('\\n')\n",
    "\n",
    "    except Exception as e: print(e)\n",
    "\n",
    "    import shutil\n",
    "    import os\n",
    "    # Specify the directory path\n",
    "    directory_path1 = '/home/aliya277/.model-tools'\n",
    "    directory_path2 = '/home/aliya277/.result_caching'\n",
    "    for directory_path in [directory_path1, directory_path2]:\n",
    "        # Check if the directory exists\n",
    "        if os.path.exists(directory_path):\n",
    "            shutil.rmtree(directory_path)\n",
    "            print(f\"Directory {directory_path} has been removed successfully.\")\n",
    "        else:\n",
    "            print(f\"Directory {directory_path} does not exist.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
