{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sachi's PSTH into correct shape and save in inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import shutil \n",
    "from natsort import natsorted\n",
    "\n",
    "# Define a list of image sets to be processed\n",
    "image_sets = ['hvm', 'bold5000', 'nat300'] #'hvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path already exists for exp_hvm.sub_solo.20181102_114336.proc\n",
      "path already exists for exp_hvm.sub_solo.20181105_112735.proc\n",
      "path already exists for exp_hvm.sub_solo.20181106_101732.proc\n",
      "path already exists for exp_hvm.sub_solo.20181107_094221.proc\n",
      "path already exists for exp_hvm.sub_solo.20181109_100301.proc\n",
      "path already exists for exp_hvm.sub_solo.20181112_105935.proc\n",
      "path already exists for exp_hvm.sub_solo.20181113_110917.proc\n",
      "path already exists for exp_hvm.sub_solo.20181114_101205.proc\n",
      "path already exists for exp_hvm.sub_solo.20181116_100519.proc\n",
      "path already exists for exp_hvm.sub_solo.20181119_095618.proc\n",
      "path already exists for exp_hvm.sub_solo.20181120_100252.proc\n",
      "path already exists for exp_hvm.sub_solo.20181121_111735.proc\n",
      "path already exists for exp_hvm.sub_solo.20181124_111612.proc\n",
      "path already exists for exp_hvm.sub_solo.20181125_110844.proc\n",
      "path already exists for exp_hvm.sub_solo.20181126_101025.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190220_143521.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190220_160047.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190221_095435.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190222_120151.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190225_111537.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190226_094812.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190227_122653.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190304_120611.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190305_112242.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190306_122612.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190307_132706.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190308_120834.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190311_113347.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190312_130545.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190317_115104.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190318_114005.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190319_115956.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190322_114704.proc\n",
      "path already exists for exp_bold5000.sub_solo.20190326_100317.proc\n",
      "path already exists for exp_nat300.sub_solo.20190218_110835.proc\n",
      "path already exists for exp_nat300.sub_solo.20190218_121915.proc\n",
      "path already exists for exp_nat300.sub_solo.20190219_100920.proc\n",
      "path already exists for exp_nat300.sub_solo.20190219_113312.proc\n",
      "path already exists for exp_nat300.sub_solo.20190228_153009.proc\n",
      "path already exists for exp_nat300.sub_solo.20190228_171728.proc\n",
      "path already exists for exp_nat300.sub_solo.20190301_123553.proc\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Iterate though every imageset and get all correct shaped psth.\n",
    "#--------------------------------------------------------------------------------------------\n",
    "for image_set in image_sets:\n",
    "    # Define paths for Intan and MWorks processed data\n",
    "    intan_proc_path = f'/braintree/data2/active/users/sachis/projects/{image_set}/monkeys/solo/intanproc'\n",
    "    mworks_proc_path = f'/braintree/data2/active/users/sachis/projects/{image_set}/monkeys/solo/mworksproc'\n",
    "\n",
    "    # List and sort Intan & MWorks recordings, ignoring hidden files\n",
    "    intan_recordings = os.listdir(intan_proc_path)\n",
    "    intan_recordings = sorted([item for item in intan_recordings if not item.startswith('.')])\n",
    "    mworks_recordings = os.listdir(mworks_proc_path)\n",
    "    mworks_recordings = sorted([item for item in mworks_recordings if not item.startswith('.')])\n",
    "\n",
    "    # Initialize lists to store dates and times\n",
    "    intan_dates = []\n",
    "    intan_times = []\n",
    "    mworks_dates = []\n",
    "    mworks_times = []\n",
    "\n",
    "    # Extract dates and times from Intan & MWorks recording filenames\n",
    "    for recording in intan_recordings:\n",
    "        intan_dates.append(recording.split('_')[2])\n",
    "        intan_times.append(recording.split('_')[3])\n",
    "    for recording in mworks_recordings:\n",
    "        mworks_dates.append(recording.split('_')[2])\n",
    "        mworks_times.append(recording.split('_')[3])\n",
    "\n",
    "    # Process each Intan recording\n",
    "    for intan_rec, intan_date, intan_time in zip(intan_recordings, intan_dates, intan_times):\n",
    "        \n",
    "        inventory_save_path = f'/braintree/home/aliya277/inventory_new/exp_{image_set}/exp_{image_set}.sub_solo/exp_{image_set}.sub_solo.20{intan_date}/exp_{image_set}.sub_solo.20{intan_date}_{intan_time}.proc'\n",
    "        if os.path.isdir(inventory_save_path): \n",
    "            print(f'path already exists for exp_{image_set}.sub_solo.20{intan_date}_{intan_time}.proc')\n",
    "            continue\n",
    "\n",
    "        # Find matching MWorks recording based on the date\n",
    "        index = mworks_dates.index(intan_date)\n",
    "        if int(mworks_times[index][:2]) <= int(intan_time[:2])-1:\n",
    "            if int(mworks_times[index+1][0:2]) <= int(intan_time[0:2])+1:\n",
    "                index = index+1\n",
    "        print(index)\n",
    "\n",
    "        mworks_rec = mworks_recordings[index]\n",
    "\n",
    "        # Load MWorks data file\n",
    "        mwk_file = sio.loadmat(os.path.join(mworks_proc_path, mworks_rec))\n",
    "        # Extract image order and unique image IDs\n",
    "        image_order = mwk_file['image_order'][0]\n",
    "        image_ids = np.unique(image_order)\n",
    "        # Determine index of each image ID in the order\n",
    "        image_id_by_index = [np.where(image_order == element)[0] for element in image_ids]\n",
    "        \n",
    "        # Find the maximum repetition count for any image\n",
    "        max_rep = max(len(arr) for arr in image_id_by_index)\n",
    "\n",
    "        # List and sort PSTH files for the current Intan recording\n",
    "        psth_files = os.listdir(os.path.join(intan_proc_path, intan_rec, 'psth'))\n",
    "        psth_files = sorted([item for item in psth_files if not item.startswith('.')])\n",
    "\n",
    "        # Populate the final PSTH array\n",
    "        for i_channel, psth_file in zip(range(len(psth_files)), psth_files):\n",
    "            psth_path = os.path.join(os.path.join(intan_proc_path, intan_rec, 'psth', psth_file))\n",
    "            psth_ = sio.loadmat(psth_path)\n",
    "            psth = psth_['psth']\n",
    "            meta = psth_['meta']\n",
    "\n",
    "            final_shape = (image_ids.shape[0], max_rep, psth.shape[1]-1)\n",
    "            final_psth = np.full(final_shape, np.nan)\n",
    "            \n",
    "\n",
    "            for image_id, image_id_indices in zip(image_ids, image_id_by_index):\n",
    "                # Assign PSTH data to the final array (image_id starts at 1)\n",
    "                final_psth[image_id-1, :len(image_id_indices), :] = psth[image_id_indices, :-1]\n",
    "        \n",
    "            # Print the shape of the final PSTH array for verification\n",
    "            print(f'PSTH for channel {i_channel} for {intan_rec} of shape {final_psth.shape}')\n",
    "\n",
    "            # Prepare data for saving\n",
    "            final_psth = final_psth.astype(np.float16) \n",
    "            data = {'psth': final_psth, 'meta': meta}\n",
    "\n",
    "            # Define the save path for the final data\n",
    "            savepath = f'/braintree/home/aliya277/inventory_new/exp_{image_set}/exp_{image_set}.sub_solo/exp_{image_set}.sub_solo.20{intan_date}/exp_{image_set}.sub_solo.20{intan_date}_{intan_time}.proc/psth/channel_files'\n",
    "            \n",
    "            if i_channel == 0:\n",
    "                try: shutil.rmtree(savepath)\n",
    "                except Exception as e: print(e)\n",
    "\n",
    "            if not os.path.isdir(savepath): os.mkdir(savepath)\n",
    "           \n",
    "            # Save the processed data in MATLAB format    \n",
    "            try: sio.savemat(os.path.join(savepath, f'{intan_rec}_channel_{i_channel}_psth.mat'), data)\n",
    "            except Exception as e: print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For Solo, the normalizer images are the last 25 images in the data (i.e., first X images are experiment images, then a grey image, and last 25 normalizers). \n",
    "It was only beginning from Oleo that we started running normalizers separately (at the beginning of each session, min. 10 repetitions). Each session has psth_meta = -100, 380, 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------\n",
    "# Iterate though every imageset and save normalizer and experiment psth in .npy files (because .mat files are too big)\n",
    "#--------------------------------------------------------------------------------------------\n",
    "\n",
    "# Loop through each image set\n",
    "for image_set in image_sets:\n",
    "    # Define paths for Intan and MWorks processed data\n",
    "    intan_proc_path = f'/braintree/data2/active/users/sachis/projects/{image_set}/monkeys/solo/intanproc'\n",
    "    mworks_proc_path = f'/braintree/data2/active/users/sachis/projects/{image_set}/monkeys/solo/mworksproc'\n",
    "\n",
    "    # List and sort Intan & MWorks recordings, ignoring hidden files\n",
    "    intan_recordings = os.listdir(intan_proc_path)\n",
    "    intan_recordings = sorted([item for item in intan_recordings if not item.startswith('.')])\n",
    "    mworks_recordings = os.listdir(mworks_proc_path)\n",
    "    mworks_recordings = sorted([item for item in mworks_recordings if not item.startswith('.')])\n",
    "\n",
    "    # Initialize lists to store dates and times\n",
    "    intan_dates = []\n",
    "    intan_times = []\n",
    "    mworks_dates = []\n",
    "    mworks_times = []\n",
    "\n",
    "    # Extract dates and times from Intan & MWorks recording filenames\n",
    "    for recording in intan_recordings:\n",
    "        intan_dates.append(recording.split('_')[2])\n",
    "        intan_times.append(recording.split('_')[3])\n",
    "    for recording in mworks_recordings:\n",
    "        mworks_dates.append(recording.split('_')[2])\n",
    "        mworks_times.append(recording.split('_')[3])\n",
    "\n",
    "    # Assuming all necessary variables (intan_recordings, intan_dates, intan_times, image_set) are defined\n",
    "    savepath_template = '/braintree/home/aliya277/inventory_new/exp_{}/exp_{}.sub_solo/exp_{}.sub_solo.20{}/exp_{}.sub_solo.20{}_{}.proc/psth/channel_files'\n",
    "    normpath_template = '/braintree/home/aliya277/inventory_new/norm_HVM/norm_HVM.sub_solo/norm_HVM.sub_solo.20{}/norm_HVM.sub_solo.20{}_{}.proc/psth'\n",
    "    final_psth_template ='{}_{}.sub_solo.20{}_{}_psth.npy'\n",
    "\n",
    "    # Preprocess all paths first\n",
    "    savepaths = [savepath_template.format(image_set, image_set, image_set, date, image_set, date, time) for date, time in zip(intan_dates, intan_times)]\n",
    "    channel_files_list = [natsorted(os.listdir(path)) for path in savepaths if os.path.isdir(path)]\n",
    "       \n",
    "    for i, path in enumerate(savepaths):\n",
    "        #if os.path.isfile(os.path.join(os.path.dirname(path), final_psth_template.format('exp', image_set, intan_dates[i], intan_times[i]))): continue \n",
    "\n",
    "        print(f'Session {os.path.basename(os.path.dirname(os.path.dirname(path)))}')\n",
    "        # Determine the dimensions\n",
    "        first_file_path = os.path.join(path, channel_files_list[i][0])\n",
    "        first_file = sio.loadmat(first_file_path)\n",
    "        num_channels = len(channel_files_list[i])  # Number of channel files per session\n",
    "        num_stim = first_file['psth'].shape[0] - 26  # Assuming 'psth' is 2D: (num_bins + 26, ...)\n",
    "        num_reps = first_file['psth'].shape[1]\n",
    "        num_bins = first_file['psth'].shape[2]\n",
    "        meta     = first_file['meta']\n",
    "        experiment_shape = (num_stim, num_reps, num_bins, num_channels)\n",
    "        normalizer_shape = (26, num_reps, num_bins, num_channels)\n",
    "\n",
    "        # Preallocate arrays\n",
    "        experiment_psth = np.empty(experiment_shape, dtype=np.float16)\n",
    "        experiment_psth[:] = np.nan\n",
    "        normalizer_psth = np.empty(normalizer_shape, dtype=np.float16)\n",
    "        normalizer_psth[:] = np.nan\n",
    "\n",
    "\n",
    "        for j, channelfile in enumerate(natsorted(channel_files_list[i])):\n",
    "            #print(path, channelfile)\n",
    "            psth_data = sio.loadmat(os.path.join(path, channelfile))['psth']\n",
    "            # Split psth data\n",
    "            experiment_psth[:,:,:, j] = psth_data[:-26]  # Experiment PSTH\n",
    "            # For the normalizer, move the first item to the end (i.e. move grey image to end to match standard format)\n",
    "            normalizer_psth[:,:,:, j] = np.roll(psth_data[-26:], shift=-1, axis=0)\n",
    "        \n",
    "        print('Experiment PSTH shape:', experiment_psth.shape)\n",
    "        print('Normalizer PSTH shape:', normalizer_psth.shape)\n",
    "\n",
    "        exp_data  = {'psth': experiment_psth, 'meta': meta}\n",
    "        norm_data = {'psth': normalizer_psth, 'meta': meta}\n",
    "\n",
    "        print('... saving exp data')\n",
    "        try: np.save(os.path.join(os.path.dirname(path), final_psth_template.format('exp', image_set, intan_dates[i], intan_times[i])), exp_data)\n",
    "        except Exception as e: print(e)\n",
    "        print('... saving norm data')\n",
    "        try: np.save(os.path.join(normpath_template.format(intan_dates[i], intan_dates[i], intan_times[i]), final_psth_template.format('norm', image_set, intan_dates[i], intan_times[i])), norm_data)\n",
    "        except Exception as e: print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the processing of this data is going to be done by the standard pipeline notebooks, starting with 1_create_proc_nwb.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dandibs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
